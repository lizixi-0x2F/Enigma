#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import re
from collections import Counter
from pathlib import Path
import jieba
from transformers import BertTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast
from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors
from tokenizers.normalizers import NFD, Lowercase, StripAccents

class FullTokenizerBuilder:
    """æ„å»ºæ»¡è¡€tokenizer - ä»çœŸå®æ•°æ®ä¸­æå–è¯­ä¹‰è¯æ±‡"""
    
    def __init__(self, vocab_size=21128):
        self.vocab_size = vocab_size
        self.texts = []
        
    def extract_texts_from_sft_data(self, sft_file="sft_data.jsonl"):
        """ä»SFTæ•°æ®ä¸­æå–æ‰€æœ‰æ–‡æœ¬"""
        print(f"ğŸ“– ä» {sft_file} æå–æ–‡æœ¬...")
        
        texts = []
        with open(sft_file, 'r', encoding='utf-8') as f:
            for line_no, line in enumerate(f, 1):
                try:
                    data = json.loads(line.strip())
                    
                    # æå–instruction, input, output
                    instruction = data.get('instruction', '')
                    input_text = data.get('input', '')
                    output = data.get('output', '')
                    
                    # æ¸…ç†æ€ç»´é“¾å†…å®¹ï¼ˆ<think>...</think>ï¼‰
                    output_clean = re.sub(r'<think>.*?</think>', '', output, flags=re.DOTALL)
                    
                    # æ”¶é›†æ‰€æœ‰æ–‡æœ¬
                    if instruction: texts.append(instruction)
                    if input_text: texts.append(input_text)
                    if output_clean: texts.append(output_clean)
                    
                    if line_no % 10000 == 0:
                        print(f"  å·²å¤„ç† {line_no} è¡Œ...")
                        
                except Exception as e:
                    print(f"âš ï¸ ç¬¬{line_no}è¡Œè§£æé”™è¯¯: {e}")
                    continue
        
        print(f"âœ… æå–å®Œæˆï¼Œå…± {len(texts)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
        self.texts = texts
        return texts
    
    def build_wordpiece_tokenizer(self):
        """æ„å»ºWordPiece tokenizerï¼ˆç±»ä¼¼BERTï¼‰"""
        print("ğŸ”§ æ„å»ºWordPiece tokenizer...")
        
        # åˆå§‹åŒ–tokenizer
        tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))
        
        # é¢„å¤„ç†å™¨ - åˆ†å‰²ç©ºæ ¼å’Œæ ‡ç‚¹
        tokenizer.pre_tokenizer = pre_tokenizers.Sequence([
            pre_tokenizers.WhitespaceSplit(),
            pre_tokenizers.Punctuation()
        ])
        
        # è®­ç»ƒå™¨
        trainer = trainers.WordPieceTrainer(
            vocab_size=self.vocab_size,
            special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]", 
                           "<s>", "</s>", "[INST]", "[/INST]", "<think>", "</think>"],
            min_frequency=2,
            continuing_subword_prefix="##"
        )
        
        # è®­ç»ƒtokenizer
        print("ğŸ“š å¼€å§‹è®­ç»ƒtokenizer...")
        tokenizer.train_from_iterator(self.texts, trainer)
        
        # è®¾ç½®è§£ç å™¨
        tokenizer.decoder = decoders.WordPiece()
        
        # è®¾ç½®åå¤„ç†å™¨
        tokenizer.post_processor = processors.BertProcessing(
            sep=("[SEP]", tokenizer.token_to_id("[SEP]")),
            cls=("[CLS]", tokenizer.token_to_id("[CLS]"))
        )
        
        print("âœ… WordPiece tokenizeræ„å»ºå®Œæˆ")
        return tokenizer
    
    def build_bpe_tokenizer(self):
        """æ„å»ºBPE tokenizer"""
        print("ğŸ”§ æ„å»ºBPE tokenizer...")
        
        # åˆå§‹åŒ–BPE tokenizer
        tokenizer = Tokenizer(models.BPE(unk_token="<unk>"))
        
        # é¢„å¤„ç†å™¨
        tokenizer.pre_tokenizer = pre_tokenizers.Sequence([
            pre_tokenizers.WhitespaceSplit(),
            pre_tokenizers.ByteLevel(add_prefix_space=False)
        ])
        
        # è®­ç»ƒå™¨
        trainer = trainers.BpeTrainer(
            vocab_size=self.vocab_size,
            special_tokens=["<unk>", "<s>", "</s>", "[INST]", "[/INST]", 
                           "<think>", "</think>", "<pad>"],
            min_frequency=2
        )
        
        # è®­ç»ƒ
        print("ğŸ“š å¼€å§‹è®­ç»ƒBPE tokenizer...")
        tokenizer.train_from_iterator(self.texts, trainer)
        
        # è§£ç å™¨
        tokenizer.decoder = decoders.ByteLevel()
        
        print("âœ… BPE tokenizeræ„å»ºå®Œæˆ")
        return tokenizer
    
    def build_char_level_tokenizer(self):
        """æ„å»ºå­—ç¬¦çº§tokenizerï¼ˆé€‚åˆä¸­æ–‡ï¼‰"""
        print("ğŸ”§ æ„å»ºå­—ç¬¦çº§tokenizer...")
        
        # ç»Ÿè®¡æ‰€æœ‰å­—ç¬¦
        char_counter = Counter()
        for text in self.texts:
            char_counter.update(text)
        
        # æ„å»ºè¯æ±‡è¡¨
        special_tokens = ["<unk>", "<s>", "</s>", "[INST]", "[/INST]", 
                         "<think>", "</think>", "<pad>"]
        
        # å–é«˜é¢‘å­—ç¬¦
        most_common_chars = char_counter.most_common(self.vocab_size - len(special_tokens))
        
        vocab = {}
        # æ·»åŠ ç‰¹æ®Štoken
        for i, token in enumerate(special_tokens):
            vocab[token] = i
        
        # æ·»åŠ å­—ç¬¦
        for char, freq in most_common_chars:
            if char not in vocab:
                vocab[char] = len(vocab)
        
        print(f"âœ… å­—ç¬¦çº§tokenizeræ„å»ºå®Œæˆï¼Œè¯æ±‡é‡: {len(vocab)}")
        return vocab
    
    def save_tokenizer(self, tokenizer, tokenizer_type, save_dir="enigma_tokenizer"):
        """ä¿å­˜tokenizer"""
        save_path = Path(save_dir)
        save_path.mkdir(exist_ok=True)
        
        if tokenizer_type in ["wordpiece", "bpe"]:
            # ä¿å­˜HuggingFaceæ ¼å¼
            tokenizer.save(str(save_path / "tokenizer.json"))
            
            # è½¬æ¢ä¸ºHuggingFace tokenizer
            hf_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
            hf_tokenizer.save_pretrained(save_path)
            
        elif tokenizer_type == "char":
            # ä¿å­˜å­—ç¬¦çº§è¯æ±‡è¡¨
            with open(save_path / "vocab.json", 'w', encoding='utf-8') as f:
                json.dump(tokenizer, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Tokenizerå·²ä¿å­˜åˆ°: {save_path}")
        return save_path
    
    def test_tokenizer(self, tokenizer, tokenizer_type):
        """æµ‹è¯•tokenizer"""
        print("\nğŸ§ª æµ‹è¯•tokenizer...")
        
        test_texts = [
            "ä½ å¥½ï¼Œæˆ‘æ˜¯Enigma",
            "1+1ç­‰äºå‡ ï¼Ÿ",
            "[INST] è¯·è§£é‡Šäººå·¥æ™ºèƒ½ [/INST]",
            "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­ï¼ŒåŒ…å«ä¸­æ–‡å’ŒEnglishæ··åˆå†…å®¹ã€‚"
        ]
        
        for text in test_texts:
            print(f"\nğŸ“ åŸæ–‡: {text}")
            
            if tokenizer_type in ["wordpiece", "bpe"]:
                encoded = tokenizer.encode(text)
                tokens = encoded.tokens
                ids = encoded.ids
                decoded = tokenizer.decode(ids)
                
                print(f"ğŸ”¢ Token IDs: {ids[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"ğŸ¯ Tokens: {tokens[:10]}...")
                print(f"ğŸ“– è§£ç : {decoded}")
                
            elif tokenizer_type == "char":
                ids = [tokenizer.get(char, tokenizer.get("<unk>", 0)) for char in text]
                chars = [k for k, v in tokenizer.items() if v in ids[:10]]
                
                print(f"ğŸ”¢ Char IDs: {ids[:10]}...")
                print(f"ğŸ“– å¯¹åº”å­—ç¬¦: {chars}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ„å»ºEnigmaæ»¡è¡€Tokenizer")
    print("=" * 60)
    
    builder = FullTokenizerBuilder(vocab_size=21128)
    
    # 1. æå–è®­ç»ƒæ•°æ®æ–‡æœ¬
    texts = builder.extract_texts_from_sft_data()
    
    if not texts:
        print("âŒ æ²¡æœ‰æå–åˆ°æ–‡æœ¬æ•°æ®ï¼")
        return
    
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»æ–‡æœ¬æ•°: {len(texts)}")
    print(f"  æ€»å­—ç¬¦æ•°: {sum(len(text) for text in texts):,}")
    print(f"  å¹³å‡é•¿åº¦: {sum(len(text) for text in texts) / len(texts):.1f}")
    
    # 2. æ„å»ºä¸åŒç±»å‹çš„tokenizer
    print("\n" + "=" * 60)
    
    # é€‰æ‹©tokenizerç±»å‹
    tokenizer_type = input("é€‰æ‹©tokenizerç±»å‹ (1: WordPiece, 2: BPE, 3: å­—ç¬¦çº§) [é»˜è®¤: 3]: ").strip()
    
    if tokenizer_type == "1":
        tokenizer = builder.build_wordpiece_tokenizer()
        save_path = builder.save_tokenizer(tokenizer, "wordpiece")
        builder.test_tokenizer(tokenizer, "wordpiece")
        
    elif tokenizer_type == "2":
        tokenizer = builder.build_bpe_tokenizer()
        save_path = builder.save_tokenizer(tokenizer, "bpe")
        builder.test_tokenizer(tokenizer, "bpe")
        
    else:  # é»˜è®¤å­—ç¬¦çº§
        tokenizer = builder.build_char_level_tokenizer()
        save_path = builder.save_tokenizer(tokenizer, "char")
        builder.test_tokenizer(tokenizer, "char")
    
    print(f"\nğŸ‰ æ»¡è¡€tokenizeræ„å»ºå®Œæˆï¼")
    print(f"ğŸ“ ä¿å­˜ä½ç½®: {save_path}")
    print("ğŸ’¡ ç°åœ¨å¯ä»¥ç”¨è¿™ä¸ªtokenizeré‡æ–°è®­ç»ƒæ¨¡å‹æˆ–è¿›è¡Œæ¨ç†")

if __name__ == "__main__":
    main() 