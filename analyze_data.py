#!/usr/bin/env python3
"""
åˆ†æwiki-full-zhæ•°æ®é›†çš„è§„æ¨¡å’Œå¤„ç†æƒ…å†µ
"""

import pandas as pd
import torch
import os
from pathlib import Path

def analyze_raw_data():
    """åˆ†æåŸå§‹parquetæ–‡ä»¶"""
    print("ğŸ” åˆ†æåŸå§‹parquetæ–‡ä»¶...")
    
    parquet_files = sorted([f for f in os.listdir('wiki-full-zh') if f.endswith('.parquet')])
    total_rows = 0
    total_size_mb = 0
    
    for file in parquet_files:
        file_path = f'wiki-full-zh/{file}'
        df = pd.read_parquet(file_path)
        file_size = os.path.getsize(file_path) / 1024 / 1024
        
        print(f"   {file}: {len(df):,} è¡Œ, {file_size:.1f}MB")
        total_rows += len(df)
        total_size_mb += file_size
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®æ€»è®¡: {total_rows:,} è¡Œ, {total_size_mb:.1f}MB")
    return total_rows

def analyze_processed_data():
    """åˆ†æå¤„ç†åçš„æ•°æ®"""
    print("\nğŸ” åˆ†æå¤„ç†åçš„æ•°æ®...")
    
    processed_file = 'wiki-full-zh/processed/processed_samples_seq256.pt'
    if os.path.exists(processed_file):
        data = torch.load(processed_file, map_location='cpu')
        file_size = os.path.getsize(processed_file) / 1024 / 1024
        
        print(f"   å¤„ç†åæ ·æœ¬æ•°: {len(data):,}")
        print(f"   æ ·æœ¬å½¢çŠ¶: {data[0].shape}")
        print(f"   æ•°æ®ç±»å‹: {data[0].dtype}")
        print(f"   æ–‡ä»¶å¤§å°: {file_size:.1f}MB")
        print(f"   è¯æ±‡è¡¨èŒƒå›´: {data[0].min().item()} - {data[0].max().item()}")
        
        return len(data)
    else:
        print("   âŒ å¤„ç†åæ–‡ä»¶ä¸å­˜åœ¨")
        return 0

def check_sample_content():
    """æ£€æŸ¥æ ·æœ¬å†…å®¹"""
    print("\nğŸ” æ£€æŸ¥æ ·æœ¬å†…å®¹...")
    
    # æ£€æŸ¥ç¬¬ä¸€ä¸ªparquetæ–‡ä»¶çš„å†…å®¹
    df = pd.read_parquet('wiki-full-zh/train-00000-of-00006.parquet')
    print(f"   åŸå§‹æ•°æ®åˆ—: {list(df.columns)}")
    
    if 'text' in df.columns:
        sample_text = df['text'].iloc[0]
        print(f"   æ ·æœ¬æ–‡æœ¬é•¿åº¦: {len(sample_text)} å­—ç¬¦")
        print(f"   æ ·æœ¬æ–‡æœ¬é¢„è§ˆ: {sample_text[:200]}...")

def main():
    print("ğŸ“ˆ Enigmaæ•°æ®é›†åˆ†ææŠ¥å‘Š")
    print("=" * 50)
    
    # åˆ†æåŸå§‹æ•°æ®
    total_raw = analyze_raw_data()
    
    # åˆ†æå¤„ç†åæ•°æ®
    total_processed = analyze_processed_data()
    
    # æ£€æŸ¥æ ·æœ¬å†…å®¹
    check_sample_content()
    
    # è®¡ç®—å¤„ç†æ¯”ä¾‹
    print("\nğŸ“‹ æ•°æ®å¤„ç†æ€»ç»“:")
    if total_raw > 0 and total_processed > 0:
        ratio = (total_processed / total_raw) * 100
        print(f"   å¤„ç†æ¯”ä¾‹: {ratio:.2f}% ({total_processed:,} / {total_raw:,})")
        print(f"   æœªå¤„ç†æ•°æ®: {total_raw - total_processed:,} è¡Œ")
    
    # ç»™å‡ºå»ºè®®
    print("\nğŸ’¡ æ‰©å±•å»ºè®®:")
    if total_raw > total_processed:
        print(f"   ğŸ¯ å¯ä»¥å¤„ç†æ›´å¤šæ•°æ®ï¼šè¿˜æœ‰ {total_raw - total_processed:,} è¡Œæœªå¤„ç†")
        print(f"   ğŸš€ å»ºè®®å¤„ç†å…¨éƒ¨æ•°æ®ä»¥æå‡æ¨¡å‹æ€§èƒ½")
        print(f"   ğŸ’¾ é¢„ä¼°å…¨é‡å¤„ç†åæ–‡ä»¶å¤§å°: ~{(total_raw / total_processed) * 202:.0f}MB")

if __name__ == "__main__":
    main() 