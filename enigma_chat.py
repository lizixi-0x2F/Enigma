#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import torch
import json
from pathlib import Path

# æ·»åŠ enigmaæ¨¡å—åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from transformers import PreTrainedTokenizer
from enigma.modeling_enigma import EnigmaConfig, EnigmaForCausalLM

# ğŸ”¥ å®Œæ•´Enigma Tokenizer
class EnigmaTokenizer(PreTrainedTokenizer):
    """åŸºäºçœŸå®æ•°æ®è®­ç»ƒçš„Enigma Tokenizer"""
    
    def __init__(self, vocab_file="enigma_tokenizer/vocab.json", **kwargs):
        # åŠ è½½è¯æ±‡è¡¨
        with open(vocab_file, 'r', encoding='utf-8') as f:
            self._vocab = json.load(f)
        
        self._id_to_token = {v: k for k, v in self._vocab.items()}
        
        # è®¾ç½®ç‰¹æ®Štoken
        super().__init__(
            unk_token="<unk>",
            bos_token="<s>",
            eos_token="</s>",
            pad_token="<pad>",
            **kwargs
        )
    
    @property
    def vocab_size(self):
        return len(self._vocab)
    
    def _tokenize(self, text):
        """å­—ç¬¦çº§åˆ†è¯"""
        return list(text)
    
    def _convert_token_to_id(self, token):
        return self._vocab.get(token, self._vocab.get("<unk>", 0))
    
    def _convert_id_to_token(self, index):
        return self._id_to_token.get(index, "<unk>")
    
    def get_vocab(self):
        return self._vocab
    
    def encode(self, text, add_special_tokens=True, max_length=None, truncation=True):
        """ç¼–ç æ–‡æœ¬ä¸ºtoken ids"""
        tokens = self._tokenize(text)
        
        if add_special_tokens:
            tokens = ["<s>"] + tokens + ["</s>"]
        
        if max_length and truncation and len(tokens) > max_length:
            tokens = tokens[:max_length]
        
        token_ids = [self._convert_token_to_id(token) for token in tokens]
        return token_ids
    
    def decode(self, token_ids, skip_special_tokens=True):
        """è§£ç token idsä¸ºæ–‡æœ¬"""
        tokens = [self._convert_id_to_token(id) for id in token_ids]
        
        if skip_special_tokens:
            special_tokens = ["<s>", "</s>", "<pad>", "<unk>"]
            tokens = [token for token in tokens if token not in special_tokens]
        
        return "".join(tokens)

class EnigmaChat:
    def __init__(self, model_path="output_full_sequence/final_model"):
        print("ğŸ”§ åˆå§‹åŒ–EnigmaèŠå¤©ç³»ç»Ÿ...")
        
        # åŠ è½½tokenizer
        print("ğŸ“– åŠ è½½Enigma Tokenizer...")
        self.tokenizer = EnigmaTokenizer()
        print(f"âœ… TokenizeråŠ è½½å®Œæˆï¼Œè¯æ±‡é‡: {self.tokenizer.vocab_size:,}")
        
        # åŠ è½½æ¨¡å‹
        print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {model_path}")
        self.config = EnigmaConfig.from_pretrained(model_path)
        self.model = EnigmaForCausalLM.from_pretrained(model_path)
        self.model.eval()
        
        # æ£€æŸ¥GPU
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        print(f"ğŸš€ æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {self.device}")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {self.model.num_parameters():,}")
        
    def generate_response(self, user_input, max_length=512, temperature=0.8, top_p=0.9):
        """ç”Ÿæˆå›å¤"""
        # æ„å»ºå¯¹è¯æ ¼å¼
        prompt = f"<s>[INST] ä½ æ˜¯Enigma\n\n{user_input} [/INST] "
        
        # ç¼–ç è¾“å…¥
        input_ids = self.tokenizer.encode(prompt, add_special_tokens=False)
        input_tensor = torch.tensor([input_ids]).to(self.device)
        
        print(f"ğŸ”¤ è¾“å…¥é•¿åº¦: {len(input_ids)} tokens")
        
        # ç”Ÿæˆå›å¤
        with torch.no_grad():
            # ç®€å•çš„è´ªå¿ƒè§£ç 
            generated_ids = input_ids.copy()
            
            for _ in range(max_length):
                # è·å–å½“å‰è¾“å…¥
                current_input = torch.tensor([generated_ids]).to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(current_input)
                logits = outputs.logits
                
                # è·å–ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒ
                next_token_logits = logits[0, -1, :]
                
                # åº”ç”¨æ¸©åº¦
                next_token_logits = next_token_logits / temperature
                
                # Top-pé‡‡æ ·
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
                
                # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„token
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                sorted_indices_to_remove[0] = 0
                
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                next_token_logits[indices_to_remove] = float('-inf')
                
                # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                probs = torch.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1).item()
                
                # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
                generated_ids.append(next_token)
                
                # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ç»“æŸtoken
                if next_token == self.tokenizer._convert_token_to_id("</s>"):
                    break
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=False)
        
        # æå–å›å¤éƒ¨åˆ†
        if "[/INST]" in generated_text:
            response = generated_text.split("[/INST]", 1)[1].strip()
            if response.endswith("</s>"):
                response = response[:-4].strip()
        else:
            response = generated_text
        
        return response
    
    def chat(self):
        """å¼€å§‹èŠå¤©å¾ªç¯"""
        print("\nğŸ‰ EnigmaèŠå¤©ç³»ç»Ÿå·²å¯åŠ¨ï¼")
        print("ğŸ’¡ è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("=" * 50)
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ ä½ : ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                if not user_input:
                    continue
                
                print("ğŸ¤– Enigmaæ­£åœ¨æ€è€ƒ...")
                response = self.generate_response(user_input)
                print(f"ğŸ¤– Enigma: {response}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {e}")
                continue

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨EnigmaèŠå¤©ç³»ç»Ÿ")
    print("=" * 50)
    
    try:
        chat_system = EnigmaChat()
        chat_system.chat()
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 