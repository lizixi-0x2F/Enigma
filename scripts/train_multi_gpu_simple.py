#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆå¤šGPUè®­ç»ƒè„šæœ¬ - ä½¿ç”¨BERTè¯æ±‡è¡¨
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.multiprocessing as mp
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import Dataset, DataLoader, random_split
from torch.utils.data.distributed import DistributedSampler
try:
    from torch.amp import autocast, GradScaler
except ImportError:
    from torch.cuda.amp import autocast, GradScaler
import time
import math
from tqdm import tqdm

# å¯¼å…¥æ¨¡å‹
import sys
sys.path.append('.')
from enigma.model import EnigmaLM

def collate_fn(batch):
    """å¯åºåˆ—åŒ–çš„collateå‡½æ•°"""
    return torch.stack(batch)

class SimpleMultiGPUDataset(Dataset):
    """ç®€åŒ–ç‰ˆå¤šGPUæ•°æ®é›†"""
    
    def __init__(self, data_path, gpu_id=0, num_gpus=5):
        print(f"ğŸš€ [GPU {gpu_id}] åŠ è½½æ•°æ®: {data_path}")
        
        # åŠ è½½æ•°æ®
        all_samples = torch.load(data_path, map_location='cpu', weights_only=False)
        
        # æ•°æ®åˆ†ç‰‡
        total_samples = len(all_samples)
        samples_per_gpu = total_samples // num_gpus
        start_idx = gpu_id * samples_per_gpu
        
        if gpu_id == num_gpus - 1:
            end_idx = total_samples
        else:
            end_idx = start_idx + samples_per_gpu
            
        self.samples = all_samples[start_idx:end_idx]
        print(f"âœ… [GPU {gpu_id}] åˆ†é…æ ·æœ¬: {start_idx}-{end_idx} ({len(self.samples)} ä¸ª)")
        
        # ä½¿ç”¨BERTè¯æ±‡è¡¨è®¾ç½®
        self.vocab_size = 21128  # BERTä¸­æ–‡è¯æ±‡è¡¨å¤§å°
        self.pad_token_id = 0    # BERTçš„[PAD] token
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        return self.samples[idx]

def setup(rank, world_size):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12357'  # æ¢ä¸ªç«¯å£
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def cleanup():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ"""
    dist.destroy_process_group()

def get_cosine_scheduler(optimizer, warmup_steps, total_steps):
    """ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        progress = (step - warmup_steps) / (total_steps - warmup_steps)
        return 0.5 * (1 + math.cos(math.pi * progress))
    
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

def train_worker(rank, world_size, config):
    """è®­ç»ƒworkerè¿›ç¨‹"""
    print(f"ğŸ¯ [GPU {rank}] å¯åŠ¨è®­ç»ƒ")
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼
    setup(rank, world_size)
    
    # åŠ è½½æ•°æ®é›†
    dataset = SimpleMultiGPUDataset(
        data_path=config['data_path'],
        gpu_id=rank,
        num_gpus=world_size
    )
    
    # æ•°æ®åˆ†ç‰‡ - 95%è®­ç»ƒï¼Œ5%éªŒè¯
    train_size = int(0.95 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        sampler=train_sampler,
        num_workers=0,
        pin_memory=True,
        collate_fn=collate_fn
    )
    
    print(f"ğŸ“Š [GPU {rank}] è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)}")
    
    # åˆ›å»ºæ¨¡å‹
    model = EnigmaLM(
        vocab_size=dataset.vocab_size,
        d=config['d_model'],
        num_rev_blocks=config['num_rev_blocks'],
        num_rotors=config['num_rotors'],
        num_transformer_layers=config['num_transformer_layers'],
        num_heads=config['num_heads'],
        max_len=2048,
        use_alibi=True,
        use_dynamic_conv1x1=True,
        conv1x1_positions=16
    ).to(rank)
    
    # DDPåŒ…è£…
    model = DDP(model, device_ids=[rank])
    
    # å¯ç”¨æ¿€æ´»æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
    if rank == 0 and config.get('use_checkpointing', True):
        print("âœ… å¯ç”¨æ¿€æ´»æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜")
    
    if rank == 0:
        total_params = sum(p.numel() for p in model.parameters())
        print(f"ğŸ“ æ¨¡å‹å‚æ•°: {total_params:,}")
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay'],
        betas=(0.9, 0.999)
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    total_steps = (len(train_loader) // config['gradient_accum_steps']) * config['max_epochs']
    warmup_steps = int(total_steps * config['warmup_ratio'])
    scheduler = get_cosine_scheduler(optimizer, warmup_steps, total_steps)
    
    # å…¶ä»–è®¾ç½®
    criterion = nn.CrossEntropyLoss(ignore_index=dataset.pad_token_id)
    try:
        scaler = GradScaler('cuda')  # æ–°ç‰ˆæœ¬API
    except:
        scaler = GradScaler()  # æ—§ç‰ˆæœ¬API
    
    if rank == 0:
        print(f"ğŸ“ˆ æ€»æ­¥æ•°: {total_steps}, é¢„çƒ­æ­¥æ•°: {warmup_steps}")
        os.makedirs(config['save_dir'], exist_ok=True)
    
    # è®­ç»ƒå¾ªç¯
    global_step = 0
    best_val_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(config['max_epochs']):
        train_sampler.set_epoch(epoch)
        model.train()
        epoch_loss = 0
        start_time = time.time()
        
        # è¿›åº¦æ¡åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤º
        if rank == 0:
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config['max_epochs']}")
        else:
            progress_bar = train_loader
        
        for step, tokens in enumerate(progress_bar):
            tokens = tokens.to(rank, non_blocking=True)
            inputs, targets = tokens[:, :-1], tokens[:, 1:]
            
            # å‰å‘ä¼ æ’­ (ä½¿ç”¨æ··åˆç²¾åº¦)
            try:
                with autocast('cuda'):  # æ–°ç‰ˆæœ¬API
                    logits = model(inputs)
                    loss = criterion(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))
                    loss = loss / config['gradient_accum_steps']
            except:
                with autocast():  # æ—§ç‰ˆæœ¬API
                    logits = model(inputs)
                    loss = criterion(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))
                    loss = loss / config['gradient_accum_steps']
            
            # åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            epoch_loss += loss.item()
            
            # å‚æ•°æ›´æ–°
            if (step + 1) % config['gradient_accum_steps'] == 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
                scheduler.step()
                optimizer.zero_grad()
                global_step += 1
                
                # éªŒè¯å’Œä¿å­˜ï¼ˆä¸»è¿›ç¨‹ï¼‰
                if rank == 0 and global_step % config['eval_steps'] == 0:
                    model.eval()
                    val_loss = 0
                    with torch.no_grad():
                        for i in range(min(50, len(val_dataset))):
                            val_tokens = val_dataset[i].unsqueeze(0).to(rank)
                            val_inputs, val_targets = val_tokens[:, :-1], val_tokens[:, 1:]
                            val_logits = model(val_inputs)
                            val_loss += criterion(val_logits.reshape(-1, val_logits.size(-1)), 
                                                val_targets.reshape(-1)).item()
                    
                    val_loss /= min(50, len(val_dataset))
                    perplexity = math.exp(val_loss)
                    print(f"\nğŸ“Š Step {global_step}, éªŒè¯æŸå¤±: {val_loss:.4f}, å›°æƒ‘åº¦: {perplexity:.2f}")
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹å¹¶æ£€æŸ¥æ—©åœ
                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        patience_counter = 0
                        torch.save({
                            'model_state_dict': model.module.state_dict(),
                            'optimizer_state_dict': optimizer.state_dict(),
                            'scheduler_state_dict': scheduler.state_dict(),
                            'global_step': global_step,
                            'val_loss': val_loss,
                            'config': config
                        }, f"{config['save_dir']}/best_model_multigpu_simple.pt")
                        print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {val_loss:.4f}, å›°æƒ‘åº¦: {perplexity:.2f})")
                    else:
                        patience_counter += 1
                        if patience_counter >= config['early_stop_patience']:
                            print(f"ğŸ”´ æ—©åœï¼šéªŒè¯æŸå¤±è¿ç»­{config['early_stop_patience']}æ¬¡æœªæ”¹å–„")
                            cleanup()
                            return
                    
                    model.train()
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆä¸»è¿›ç¨‹ï¼‰
                if rank == 0 and global_step % config['save_steps'] == 0:
                    torch.save({
                        'model_state_dict': model.module.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'epoch': epoch,
                        'global_step': global_step,
                        'config': config
                    }, f"{config['save_dir']}/checkpoint_step_{global_step}.pt")
                    print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: step_{global_step}")
        
        # è®¡ç®—å¹³å‡æŸå¤±
        epoch_loss = epoch_loss / len(train_loader) * config['gradient_accum_steps']
        epoch_time = time.time() - start_time
        
        if rank == 0:
            print(f"ğŸ¯ Epoch {epoch+1}/{config['max_epochs']} å®Œæˆ")
            print(f"   å¹³å‡è®­ç»ƒæŸå¤±: {epoch_loss:.4f}")
            print(f"   è€—æ—¶: {epoch_time:.1f}ç§’")
            print(f"   å½“å‰å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.2e}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 2 == 0:
                torch.save({
                    'model_state_dict': model.module.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'epoch': epoch,
                    'global_step': global_step,
                    'config': config
                }, f"{config['save_dir']}/checkpoint_epoch_{epoch+1}.pt")
                print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: epoch_{epoch+1}")
    
    # æ¸…ç†
    cleanup()

def main():
    # æ£€æŸ¥GPUæ•°é‡
    world_size = torch.cuda.device_count()
    print(f"ğŸš€ å¯åŠ¨ç®€åŒ–ç‰ˆå¤šGPUè®­ç»ƒ")
    print(f"ğŸ¯ æ£€æµ‹åˆ° {world_size} å¼ GPU")
    
    # è®­ç»ƒé…ç½®
    config = {
        # æ•°æ®é…ç½®
        'data_path': 'wiki-full-zh/processed/train_seq256_bert_fast.pt',
        
        # æ¨¡å‹é…ç½®
        'd_model': 512,             # ä¸­ç­‰ç»´åº¦ï¼Œå¹³è¡¡è¡¨è¾¾åŠ›ä¸è®¡ç®—é‡
        'num_transformer_layers': 8,  # 8å±‚è‡ªæ³¨æ„åŠ›ï¼Œè¶³å¤Ÿæ•æ‰ä¸­é•¿ç¨‹ä¾èµ–
        'num_heads': 8,             # æ¯å¤´ç»´åº¦64
        'num_rev_blocks': 4,        # 4å±‚å¯é€†è€¦åˆï¼Œä¿æŒéçº¿æ€§å˜æ¢èƒ½åŠ›
        'num_rotors': 2,            # 2ä¸ªè½¬å­å³å¯æä¾›åŠ¨æ€ç½®æ¢
        
        # è®­ç»ƒé…ç½® (å¤šGPUè°ƒæ•´)
        'batch_size': 16,           # æ¯GPUæ‰¹å¤§å°ï¼Œæ€»effective=16Ã—5Ã—2=160
        'learning_rate': 5e-4,      # å­¦ä¹ ç‡ç¨ä½ï¼Œæ›´ç¨³å®š
        'weight_decay': 1e-3,       # è½»åº¦æƒé‡è¡°å‡é˜²è¿‡æ‹Ÿåˆ
        'max_epochs': 5,            # æ•°æ®é‡å¤§æ—¶å°‘è·‘å‡ è½®å³å¯
        'gradient_accum_steps': 2,  # è‹¥æ˜¾å­˜ç´§å¼ ï¼Œç­‰æ•ˆbatchæ›´å¤§
        'warmup_ratio': 0.1,        # é¢„çƒ­10%ï¼Œå¿«é€Ÿè¿›å…¥æ”¶æ•›åŒºé—´
        
        # ä¼˜åŒ–è®¾ç½®
        'use_checkpointing': True,  # æ¿€æ´»æ£€æŸ¥ç‚¹ï¼Œçœä¸‹ä¸­é—´æ¿€æ´»å­˜å‚¨
        'early_stop_patience': 2,   # éªŒè¯é›†ä¸Šè¿ç»­2è½®ä¸é™å³åœ
        
        # ä¿å­˜å’ŒéªŒè¯é…ç½®
        'eval_steps': 2000,         # æ¯2kæ­¥åœ¨éªŒè¯é›†ä¸Šè®¡ç®—å›°æƒ‘åº¦
        'save_steps': 10000,        # æ¯10kæ­¥å­˜ä¸€æ¬¡ï¼Œé˜²æ­¢æ„å¤–ä¸­æ–­
        'save_dir': 'checkpoints_multigpu_simple_512d_optimized'  # åŒºåˆ†ä¿å­˜ç›®å½•
    }
    
    # è®¡ç®—effective batch size
    effective_batch_size = config['batch_size'] * world_size * config['gradient_accum_steps']
    
    print(f"ğŸ¯ è®­ç»ƒé…ç½®:")
    print(f"   d_model: {config['d_model']}")
    print(f"   num_transformer_layers: {config['num_transformer_layers']}")
    print(f"   num_heads: {config['num_heads']}")
    print(f"   num_rev_blocks: {config['num_rev_blocks']}")
    print(f"   num_rotors: {config['num_rotors']}")
    print(f"   batch_size: {config['batch_size']} (æ¯GPU)")
    print(f"   learning_rate: {config['learning_rate']}")
    print(f"   weight_decay: {config['weight_decay']}")
    print(f"   max_epochs: {config['max_epochs']}")
    print(f"   gradient_accum_steps: {config['gradient_accum_steps']}")
    print(f"   æ€»effective batch size: {effective_batch_size}")
    
    # å¯åŠ¨å¤šè¿›ç¨‹è®­ç»ƒ
    mp.spawn(train_worker, args=(world_size, config), nprocs=world_size, join=True)
    
    print("ğŸ‰ å¤šGPUè®­ç»ƒå®Œæˆ!")

if __name__ == "__main__":
    main() 