#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆå•GPUè®­ç»ƒè„šæœ¬ - ä½¿ç”¨BERTè¯æ±‡è¡¨
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
try:
    from torch.amp import autocast, GradScaler
except ImportError:
    from torch.cuda.amp import autocast, GradScaler
import time
import math
from tqdm import tqdm

# å¯¼å…¥æ¨¡å‹
import sys
sys.path.append('.')
from enigma.model import EnigmaLM

class SimpleSingleGPUDataset(Dataset):
    """ç®€åŒ–ç‰ˆå•GPUæ•°æ®é›†"""
    
    def __init__(self, data_path):
        print(f"ğŸš€ åŠ è½½æ•°æ®: {data_path}")
        
        # åŠ è½½æ•°æ®
        self.samples = torch.load(data_path, map_location='cpu', weights_only=False)
        print(f"âœ… åŠ è½½å®Œæˆï¼Œæ€»æ ·æœ¬æ•°: {len(self.samples)}")
        
        # ä½¿ç”¨BERTè¯æ±‡è¡¨è®¾ç½®
        self.vocab_size = 21128  # BERTä¸­æ–‡è¯æ±‡è¡¨å¤§å°
        self.pad_token_id = 0    # BERTçš„[PAD] token
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        return self.samples[idx]

def get_cosine_scheduler(optimizer, warmup_steps, total_steps):
    """ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        progress = (step - warmup_steps) / (total_steps - warmup_steps)
        return 0.5 * (1 + math.cos(math.pi * progress))
    
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

def train_single_gpu(config):
    """å•GPUè®­ç»ƒå‡½æ•°"""
    print(f"ğŸ¯ å¯åŠ¨å•GPUè®­ç»ƒ")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®é›†
    dataset = SimpleSingleGPUDataset(data_path=config['data_path'])
    
    # æ•°æ®åˆ†ç‰‡ - 95%è®­ç»ƒï¼Œ5%éªŒè¯
    train_size = int(0.95 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=2,
        pin_memory=True
    )
    
    print(f"ğŸ“Š è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)}")
    
    # åˆ›å»ºæ¨¡å‹
    model = EnigmaLM(
        vocab_size=dataset.vocab_size,
        d=config['d_model'],
        num_rev_blocks=config['num_rev_blocks'],
        num_rotors=config['num_rotors'],
        num_transformer_layers=config['num_transformer_layers'],
        num_heads=config['num_heads'],
        max_len=2048,
        use_alibi=True,
        use_dynamic_conv1x1=True,
        conv1x1_positions=16
    ).to(device)
    
    # å¯ç”¨æ¿€æ´»æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
    if config.get('use_checkpointing', True):
        from torch.utils.checkpoint import checkpoint
        print("âœ… å¯ç”¨æ¿€æ´»æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜")
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“ æ¨¡å‹å‚æ•°: {total_params:,}")
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay'],
        betas=(0.9, 0.999)
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    total_steps = (len(train_loader) // config['gradient_accum_steps']) * config['max_epochs']
    warmup_steps = int(total_steps * config['warmup_ratio'])
    scheduler = get_cosine_scheduler(optimizer, warmup_steps, total_steps)
    
    # å…¶ä»–è®¾ç½®
    criterion = nn.CrossEntropyLoss(ignore_index=dataset.pad_token_id)
    try:
        scaler = GradScaler('cuda')  # æ–°ç‰ˆæœ¬API
    except:
        scaler = GradScaler()  # æ—§ç‰ˆæœ¬API
    
    print(f"ğŸ“ˆ æ€»æ­¥æ•°: {total_steps}, é¢„çƒ­æ­¥æ•°: {warmup_steps}")
    os.makedirs(config['save_dir'], exist_ok=True)
    
    # è®­ç»ƒå¾ªç¯
    global_step = 0
    best_val_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(config['max_epochs']):
        model.train()
        epoch_loss = 0
        start_time = time.time()
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config['max_epochs']}")
        
        for step, tokens in enumerate(progress_bar):
            tokens = tokens.to(device, non_blocking=True)
            inputs, targets = tokens[:, :-1], tokens[:, 1:]
            
            # å‰å‘ä¼ æ’­ (ä½¿ç”¨æ··åˆç²¾åº¦)
            try:
                with autocast('cuda'):  # æ–°ç‰ˆæœ¬API
                    logits = model(inputs)
                    loss = criterion(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))
                    loss = loss / config['gradient_accum_steps']
            except:
                with autocast():  # æ—§ç‰ˆæœ¬API
                    logits = model(inputs)
                    loss = criterion(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))
                    loss = loss / config['gradient_accum_steps']
            
            # åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            epoch_loss += loss.item()
            
            # å‚æ•°æ›´æ–°
            if (step + 1) % config['gradient_accum_steps'] == 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
                scheduler.step()
                optimizer.zero_grad()
                global_step += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                current_lr = scheduler.get_last_lr()[0]
                progress_bar.set_postfix({
                    'loss': f'{loss.item() * config["gradient_accum_steps"]:.4f}',
                    'lr': f'{current_lr:.2e}',
                    'step': global_step
                })
                
                # éªŒè¯å’Œä¿å­˜
                if global_step % config['eval_steps'] == 0:
                    model.eval()
                    val_loss = 0
                    val_steps = 0
                    
                    with torch.no_grad():
                        for val_tokens in val_loader:
                            if val_steps >= 50:  # é™åˆ¶éªŒè¯æ­¥æ•°
                                break
                            val_tokens = val_tokens.to(device)
                            val_inputs, val_targets = val_tokens[:, :-1], val_tokens[:, 1:]
                            val_logits = model(val_inputs)
                            val_loss += criterion(val_logits.reshape(-1, val_logits.size(-1)), 
                                                val_targets.reshape(-1)).item()
                            val_steps += 1
                    
                    val_loss /= val_steps
                    perplexity = math.exp(val_loss)
                    print(f"\nğŸ“Š Step {global_step}, éªŒè¯æŸå¤±: {val_loss:.4f}, å›°æƒ‘åº¦: {perplexity:.2f}")
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹å¹¶æ£€æŸ¥æ—©åœ
                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        patience_counter = 0
                        torch.save({
                            'model_state_dict': model.state_dict(),
                            'optimizer_state_dict': optimizer.state_dict(),
                            'scheduler_state_dict': scheduler.state_dict(),
                            'global_step': global_step,
                            'val_loss': val_loss,
                            'config': config
                        }, f"{config['save_dir']}/best_model_single_gpu.pt")
                        print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {val_loss:.4f}, å›°æƒ‘åº¦: {perplexity:.2f})")
                    else:
                        patience_counter += 1
                        if patience_counter >= config['early_stop_patience']:
                            print(f"ğŸ”´ æ—©åœï¼šéªŒè¯æŸå¤±è¿ç»­{config['early_stop_patience']}æ¬¡æœªæ”¹å–„")
                            return
                    
                    model.train()
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if global_step % config['save_steps'] == 0:
                    torch.save({
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'epoch': epoch,
                        'global_step': global_step,
                        'config': config
                    }, f"{config['save_dir']}/checkpoint_step_{global_step}.pt")
                    print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: step_{global_step}")
        
        # è®¡ç®—å¹³å‡æŸå¤±
        epoch_loss = epoch_loss / len(train_loader) * config['gradient_accum_steps']
        epoch_time = time.time() - start_time
        
        print(f"ğŸ¯ Epoch {epoch+1}/{config['max_epochs']} å®Œæˆ")
        print(f"   å¹³å‡è®­ç»ƒæŸå¤±: {epoch_loss:.4f}")
        print(f"   è€—æ—¶: {epoch_time:.1f}ç§’")
        print(f"   å½“å‰å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.2e}")
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % 2 == 0:
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'epoch': epoch,
                'global_step': global_step,
                'config': config
            }, f"{config['save_dir']}/checkpoint_epoch_{epoch+1}.pt")
            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: epoch_{epoch+1}")

def main():
    print(f"ğŸš€ å¯åŠ¨ç®€åŒ–ç‰ˆå•GPUè®­ç»ƒ")
    
    # è®­ç»ƒé…ç½®
    config = {
        # æ•°æ®é…ç½®
        'data_path': 'wiki-full-zh/processed/train_seq256_bert_fast.pt',
        
        # æ¨¡å‹é…ç½®
        'd_model': 512,             # ä¸­ç­‰ç»´åº¦ï¼Œå¹³è¡¡è¡¨è¾¾åŠ›ä¸è®¡ç®—é‡
        'num_transformer_layers': 8,  # 8å±‚è‡ªæ³¨æ„åŠ›ï¼Œè¶³å¤Ÿæ•æ‰ä¸­é•¿ç¨‹ä¾èµ–
        'num_heads': 8,             # æ¯å¤´ç»´åº¦64
        'num_rev_blocks': 4,        # 4å±‚å¯é€†è€¦åˆï¼Œä¿æŒéçº¿æ€§å˜æ¢èƒ½åŠ›
        'num_rotors': 2,            # 2ä¸ªè½¬å­å³å¯æä¾›åŠ¨æ€ç½®æ¢
        
        # è®­ç»ƒé…ç½®
        'batch_size': 64,           # æ‰¹é‡è¶Šå¤§ï¼Œååè¶Šé«˜
        'learning_rate': 5e-4,      # å­¦ä¹ ç‡ç¨ä½ï¼Œæ›´ç¨³å®š
        'weight_decay': 1e-3,       # è½»åº¦æƒé‡è¡°å‡é˜²è¿‡æ‹Ÿåˆ
        'max_epochs': 5,            # æ•°æ®é‡å¤§æ—¶å°‘è·‘å‡ è½®å³å¯
        'gradient_accum_steps': 2,  # è‹¥æ˜¾å­˜ç´§å¼ ï¼Œç­‰æ•ˆbatch=128
        'warmup_ratio': 0.1,        # é¢„çƒ­10%ï¼Œå¿«é€Ÿè¿›å…¥æ”¶æ•›åŒºé—´
        
        # ä¼˜åŒ–è®¾ç½®
        'use_checkpointing': True,  # æ¿€æ´»æ£€æŸ¥ç‚¹ï¼Œçœä¸‹ä¸­é—´æ¿€æ´»å­˜å‚¨
        'early_stop_patience': 2,   # éªŒè¯é›†ä¸Šè¿ç»­2è½®ä¸é™å³åœ
        
        # ä¿å­˜å’ŒéªŒè¯é…ç½®
        'eval_steps': 2000,         # æ¯2kæ­¥åœ¨éªŒè¯é›†ä¸Šè®¡ç®—å›°æƒ‘åº¦
        'save_steps': 10000,        # æ¯10kæ­¥å­˜ä¸€æ¬¡ï¼Œé˜²æ­¢æ„å¤–ä¸­æ–­
        'save_dir': 'checkpoints_single_gpu_512d_optimized'  # åŒºåˆ†ä¿å­˜ç›®å½•
    }
    
    # è®¡ç®—effective batch size
    effective_batch_size = config['batch_size'] * config['gradient_accum_steps']
    
    print(f"ğŸ¯ è®­ç»ƒé…ç½®:")
    print(f"   d_model: {config['d_model']}")
    print(f"   num_transformer_layers: {config['num_transformer_layers']}")
    print(f"   num_heads: {config['num_heads']}")
    print(f"   num_rev_blocks: {config['num_rev_blocks']}")
    print(f"   num_rotors: {config['num_rotors']}")
    print(f"   batch_size: {config['batch_size']}")
    print(f"   learning_rate: {config['learning_rate']}")
    print(f"   weight_decay: {config['weight_decay']}")
    print(f"   max_epochs: {config['max_epochs']}")
    print(f"   gradient_accum_steps: {config['gradient_accum_steps']}")
    print(f"   effective batch size: {effective_batch_size}")
    
    # å¯åŠ¨è®­ç»ƒ
    train_single_gpu(config)
    
    print("ğŸ‰ å•GPUè®­ç»ƒå®Œæˆ!")

if __name__ == "__main__":
    main() 