#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é˜²è¿‡æ‹Ÿåˆè®­ç»ƒè„šæœ¬ - åŸºäº300M tokensæ•°æ®å’Œè¶…å‚æ•°ä¼˜åŒ–å»ºè®®
ä¸¥æ ¼æ§åˆ¶è®­ç»ƒepochæ•°ï¼Œå¢åŠ æ­£åˆ™åŒ–ï¼Œé˜²æ­¢é‡æ–°è¿‡æ‹Ÿåˆ
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.multiprocessing as mp
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import Dataset, DataLoader, random_split
from torch.utils.data.distributed import DistributedSampler
try:
    from torch.amp import autocast, GradScaler
except ImportError:
    from torch.cuda.amp import autocast, GradScaler
import time
import math
from tqdm import tqdm

# å¯¼å…¥æ¨¡å‹
import sys
sys.path.append('.')
from enigma.model import EnigmaLM

def collate_fn(batch):
    """å¯åºåˆ—åŒ–çš„collateå‡½æ•°"""
    return torch.stack(batch)

class AntiOverfittingDataset(Dataset):
    """é˜²è¿‡æ‹Ÿåˆæ•°æ®é›† - ä½¿ç”¨ä¿®å¤åçš„æ— æ³„æ¼æ•°æ®"""
    
    def __init__(self, data_path, gpu_id=0, num_gpus=5):
        print(f"ğŸš€ [GPU {gpu_id}] åŠ è½½ä¿®å¤åçš„æ— æ³„æ¼æ•°æ®: {data_path}")
        
        # åŠ è½½ä¿®å¤åçš„æ•°æ®
        all_samples = torch.load(data_path, map_location='cpu', weights_only=False)
        
        # æ•°æ®åˆ†ç‰‡
        total_samples = len(all_samples)
        samples_per_gpu = total_samples // num_gpus
        start_idx = gpu_id * samples_per_gpu
        
        if gpu_id == num_gpus - 1:
            end_idx = total_samples
        else:
            end_idx = start_idx + samples_per_gpu
            
        self.samples = all_samples[start_idx:end_idx]
        print(f"âœ… [GPU {gpu_id}] åˆ†é…æ ·æœ¬: {start_idx}-{end_idx} ({len(self.samples)} ä¸ª)")
        
        # BERTè¯æ±‡è¡¨è®¾ç½®
        self.vocab_size = 21128  # BERTä¸­æ–‡è¯æ±‡è¡¨å¤§å°
        self.pad_token_id = 0    # BERTçš„[PAD] token
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        return self.samples[idx]

def setup(rank, world_size):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12358'  # æ–°ç«¯å£é¿å…å†²çª
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def cleanup():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ"""
    dist.destroy_process_group()

def get_cosine_scheduler_with_warmup(optimizer, warmup_steps, total_steps, min_lr_ratio=0.1):
    """å¸¦é¢„çƒ­çš„ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œæœ€ä½å­¦ä¹ ç‡ä¸ä¸º0"""
    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        progress = (step - warmup_steps) / (total_steps - warmup_steps)
        cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))
        return min_lr_ratio + (1 - min_lr_ratio) * cosine_decay
    
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

def calculate_perplexity_accurately(model, criterion, val_loader, device, max_batches=20):
    """ğŸƒ æ›´å‡†ç¡®åœ°è®¡ç®—å›°æƒ‘åº¦ - é€‚é…è¯æ–¹3çš„ä¸¥æ ¼Mask"""
    model.eval()
    total_loss = 0
    total_tokens = 0
    
    with torch.no_grad():
        for i, tokens in enumerate(val_loader):
            if i >= max_batches:
                break
                
            tokens = tokens.to(device)
            inputs, targets = tokens[:, :-1], tokens[:, 1:]
            
            logits = model(inputs)
            # ğŸƒ è¯æ–¹2+3: å¯¹é½å¹¶ä¸¥æ ¼mask
            logits = logits[:, :-1]
            targets = targets[:, 1:]
            
            flat_logits = logits.reshape(-1, logits.size(-1))
            flat_targets = targets.reshape(-1)
            mask = flat_targets != 0  # 0æ˜¯pad_token_id
            
            if mask.sum() > 0:
                loss = criterion(flat_logits[mask], flat_targets[mask]) / mask.sum()
                total_loss += loss.item() * mask.sum().item()
                total_tokens += mask.sum().item()
    
    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')
    perplexity = math.exp(avg_loss) if avg_loss < 20 else float('inf')  # é˜²æ­¢æ•°å€¼æº¢å‡º
    
    return avg_loss, perplexity

def train_worker(rank, world_size, config):
    """è®­ç»ƒworkerè¿›ç¨‹ - é˜²è¿‡æ‹Ÿåˆç‰ˆæœ¬"""
    print(f"ğŸ¯ [GPU {rank}] å¯åŠ¨é˜²è¿‡æ‹Ÿåˆè®­ç»ƒ")
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼
    setup(rank, world_size)
    
    # åŠ è½½ä¿®å¤åçš„æ•°æ®é›†
    dataset = AntiOverfittingDataset(
        data_path=config['data_path'],
        gpu_id=rank,
        num_gpus=world_size
    )
    
    # ğŸƒ è¯æ–¹1: é‡åˆ‡éªŒè¯é›† - ç¡®ä¿è¡Œçº§å»é‡ï¼Œ10%éªŒè¯é›†
    train_size = int(0.9 * len(dataset))  # 90%è®­ç»ƒï¼Œ10%éªŒè¯
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size], 
                                              generator=torch.Generator().manual_seed(42))
    
    if rank == 0:
        print(f"ğŸƒ [è¯æ–¹1] é‡åˆ‡éªŒè¯é›†: è®­ç»ƒ90% éªŒè¯10%ï¼Œç¡®ä¿æ— æ•°æ®æ³„æ¼")
    
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        sampler=train_sampler,
        num_workers=0,
        pin_memory=True,
        collate_fn=collate_fn
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=0,
        pin_memory=True,
        collate_fn=collate_fn
    )
    
    if rank == 0:
        print(f"ğŸ“Š è®­ç»ƒé›†: {len(train_dataset):,}, éªŒè¯é›†: {len(val_dataset):,}")
        print(f"ğŸ¯ æ•°æ®æ€»tokensä¼°è®¡: ~{len(train_dataset) * 256 / 1e6:.1f}M")
    
    # åˆ›å»ºæ¨¡å‹ - ä½¿ç”¨é…ç½®çš„dropoutå‚æ•°
    model = EnigmaLM(
        vocab_size=dataset.vocab_size,
        d=config['d_model'],
        num_rev_blocks=config['num_rev_blocks'],
        num_rotors=config['num_rotors'],
        num_transformer_layers=config['num_transformer_layers'],
        num_heads=config['num_heads'],
        max_len=2048,
        use_alibi=True,
        use_dynamic_conv1x1=True,
        conv1x1_positions=16
    ).to(rank)
    
    # æ‰‹åŠ¨è®¾ç½®dropoutå‚æ•°ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
    def set_dropout_recursive(module, dropout_rate):
        """é€’å½’è®¾ç½®æ¨¡å‹çš„dropoutç‡"""
        for name, child in module.named_children():
            if isinstance(child, nn.Dropout):
                child.p = dropout_rate
                print(f"ğŸ¯ [GPU {rank}] è®¾ç½® {name} dropout = {dropout_rate}")
            else:
                set_dropout_recursive(child, dropout_rate)
    
    if rank == 0:
        print(f"ğŸ¯ è®¾ç½®attentionå’ŒFFå±‚dropout = {config['attention_dropout']}")
    set_dropout_recursive(model, config['attention_dropout'])
    
    # DDPåŒ…è£…
    model = DDP(model, device_ids=[rank])
    
    if rank == 0:
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"ğŸ“ æ¨¡å‹å‚æ•°: {total_params:,} (å¯è®­ç»ƒ: {trainable_params:,})")
    
    # ä¼˜åŒ–å™¨ - åº”ç”¨weight decay
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay'],  # é‡è¦çš„æ­£åˆ™åŒ–å‚æ•°
        betas=(0.9, 0.95),  # ç¨å¾®è°ƒæ•´beta2ï¼Œæ›´é€‚åˆå¤§æ¨¡å‹
        eps=1e-8
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä¸¥æ ¼æ§åˆ¶æ€»æ­¥æ•°
    total_steps = (len(train_loader) // config['gradient_accum_steps']) * config['max_epochs']
    warmup_steps = int(total_steps * config['warmup_ratio'])
    scheduler = get_cosine_scheduler_with_warmup(optimizer, warmup_steps, total_steps, min_lr_ratio=0.1)
    
    # ğŸƒ è¯æ–¹3: ä¸¥æ ¼Mask - ä½¿ç”¨reduction='sum'åæ‰‹åŠ¨è®¡ç®—å¹³å‡
    criterion = nn.CrossEntropyLoss(ignore_index=dataset.pad_token_id, reduction='sum')
    try:
        scaler = GradScaler('cuda')
    except:
        scaler = GradScaler()
    
    if rank == 0:
        print(f"ğŸƒ [è¯æ–¹3] ä¸¥æ ¼Mask: ä½¿ç”¨reduction='sum'ç¡®ä¿åˆ†æ¯>0")
        print(f"\nğŸƒ =====ã€åŒ»ç”Ÿå¤„æ–¹å®æ–½çŠ¶æ€ã€‘=====")
        print(f"ğŸƒ è¯æ–¹1: âœ… é‡åˆ‡éªŒè¯é›† (90%è®­ç»ƒ 10%éªŒè¯)")
        print(f"ğŸƒ è¯æ–¹2: âœ… ä¿®æ­£å¯¹é½ (logits[:-1] vs labels[1:])")
        print(f"ğŸƒ è¯æ–¹3: âœ… ä¸¥æ ¼Mask (sum/count ç¡®ä¿åˆ†æ¯>0)")
        print(f"ğŸƒ è¯æ–¹4: âœ… æ—©åœé˜ˆå€¼ (PPL>{config['early_stop_ppl']}) æ¯{config['eval_steps']}æ­¥è¯„ä¼°")
        print(f"ğŸƒ è¯æ–¹5: âœ… è½»è°ƒLR ({config['learning_rate']:.0e}, cosine decay)")
        print(f"ğŸƒ è¯æ–¹6: âœ… æ­£åˆ™åŠ å‘³ (dropout={config['attention_dropout']}, decay={config['weight_decay']})")
        print(f"ğŸƒ ================================")
    
    if rank == 0:
        print(f"ğŸ“ˆ é˜²è¿‡æ‹Ÿåˆé…ç½®:")
        print(f"   max_epochs: {config['max_epochs']} (ä¸¥æ ¼é™åˆ¶!)")
        print(f"   æ€»æ­¥æ•°: {total_steps:,}")
        print(f"   é¢„çƒ­æ­¥æ•°: {warmup_steps:,}")
        print(f"   learning_rate: {config['learning_rate']}")
        print(f"   weight_decay: {config['weight_decay']}")
        print(f"   attention_dropout: {config['attention_dropout']}")
        print(f"   æ—©åœpatience: {config['early_stop_patience']}")
        os.makedirs(config['save_dir'], exist_ok=True)
    
    # è®­ç»ƒå¾ªç¯ - ä¸¥æ ¼ç›‘æ§è¿‡æ‹Ÿåˆ
    global_step = 0
    best_val_loss = float('inf')
    patience_counter = 0
    training_losses = []
    validation_losses = []
    
    for epoch in range(config['max_epochs']):
        train_sampler.set_epoch(epoch)
        model.train()
        epoch_loss = 0
        start_time = time.time()
        
        # è¿›åº¦æ¡åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤º
        if rank == 0:
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config['max_epochs']} [é˜²è¿‡æ‹Ÿåˆæ¨¡å¼]")
        else:
            progress_bar = train_loader
        
        for step, tokens in enumerate(progress_bar):
            tokens = tokens.to(rank, non_blocking=True)
            
            # ğŸƒ è¯æ–¹2: ä¿®æ­£å¯¹é½ - logitså–[:, :-1], labelså–[:, 1:]ï¼Œé¿å…"çœ‹ç­”æ¡ˆ"
            inputs, targets = tokens[:, :-1], tokens[:, 1:]
            
            # å‰å‘ä¼ æ’­ (ä½¿ç”¨æ··åˆç²¾åº¦)
            try:
                with autocast('cuda'):
                    logits = model(inputs)
                    # ğŸƒ è¯æ–¹2: ç¡®ä¿logitså’Œtargetså½¢çŠ¶å¯¹é½
                    logits = logits[:, :-1]  # å–å‰n-1ä¸ªlogits
                    targets = targets[:, 1:]  # å–ån-1ä¸ªtargets
                    
                    # ğŸƒ è¯æ–¹3: ä¸¥æ ¼Maskè®¡ç®—lossï¼Œç¡®ä¿åˆ†æ¯>0
                    flat_logits = logits.reshape(-1, logits.size(-1))
                    flat_targets = targets.reshape(-1)
                    mask = flat_targets != dataset.pad_token_id
                    
                    if mask.sum() > 0:
                        loss = criterion(flat_logits[mask], flat_targets[mask]) / mask.sum()
                    else:
                        loss = torch.tensor(0.0, device=rank, requires_grad=True)
                    loss = loss / config['gradient_accum_steps']
            except:
                with autocast():
                    logits = model(inputs)
                    # ğŸƒ è¯æ–¹2: ç¡®ä¿logitså’Œtargetså½¢çŠ¶å¯¹é½
                    logits = logits[:, :-1]  # å–å‰n-1ä¸ªlogits
                    targets = targets[:, 1:]  # å–ån-1ä¸ªtargets
                    
                    # ğŸƒ è¯æ–¹3: ä¸¥æ ¼Maskè®¡ç®—lossï¼Œç¡®ä¿åˆ†æ¯>0
                    flat_logits = logits.reshape(-1, logits.size(-1))
                    flat_targets = targets.reshape(-1)
                    mask = flat_targets != dataset.pad_token_id
                    
                    if mask.sum() > 0:
                        loss = criterion(flat_logits[mask], flat_targets[mask]) / mask.sum()
                    else:
                        loss = torch.tensor(0.0, device=rank, requires_grad=True)
                    loss = loss / config['gradient_accum_steps']
            
            # ğŸƒ è¯æ–¹2: æ¯500æ­¥æ£€æŸ¥å¯¹é½è´¨é‡
            if rank == 0 and global_step % 500 == 0 and step == 0:
                with torch.no_grad():
                    pred_tokens = torch.argmax(logits[0], dim=-1)  # ç¬¬ä¸€ä¸ªæ ·æœ¬çš„é¢„æµ‹
                    true_tokens = targets[0]  # ç¬¬ä¸€ä¸ªæ ·æœ¬çš„çœŸå®æ ‡ç­¾
                    valid_mask = true_tokens != dataset.pad_token_id
                    if valid_mask.sum() > 0:
                        accuracy = (pred_tokens[valid_mask] == true_tokens[valid_mask]).float().mean()
                        print(f"ğŸƒ [è¯æ–¹2] Step {global_step} å¯¹é½æ£€æŸ¥: argmax==label å‡†ç¡®ç‡ {accuracy:.1%}")
                        if accuracy < 0.8:
                            print("âš ï¸  å‡†ç¡®ç‡è¿‡ä½ï¼Œæ£€æŸ¥logits-targetså¯¹é½")
            
            # åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            epoch_loss += loss.item()
            
            # å‚æ•°æ›´æ–°
            if (step + 1) % config['gradient_accum_steps'] == 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
                scheduler.step()
                optimizer.zero_grad()
                global_step += 1
                
                # éªŒè¯å’Œè¿‡æ‹Ÿåˆæ£€æµ‹ï¼ˆä¸»è¿›ç¨‹ï¼‰
                if rank == 0 and global_step % config['eval_steps'] == 0:
                    val_loss, perplexity = calculate_perplexity_accurately(
                        model.module, criterion, val_loader, rank
                    )
                    
                    training_losses.append(epoch_loss / (step + 1) * config['gradient_accum_steps'])
                    validation_losses.append(val_loss)
                    
                    # è®¡ç®—è¿‡æ‹ŸåˆæŒ‡æ ‡
                    if len(training_losses) >= 2:
                        train_loss_trend = training_losses[-1] - training_losses[-2]
                        val_loss_trend = val_loss - validation_losses[-2]
                        overfitting_signal = val_loss_trend > 0 and train_loss_trend < -0.01
                    else:
                        overfitting_signal = False
                    
                    current_lr = scheduler.get_last_lr()[0]
                    print(f"\nğŸ“Š Step {global_step}")
                    print(f"   è®­ç»ƒæŸå¤±: {training_losses[-1]:.4f}")
                    print(f"   éªŒè¯æŸå¤±: {val_loss:.4f}")
                    print(f"   å›°æƒ‘åº¦: {perplexity:.2f}")
                    print(f"   å­¦ä¹ ç‡: {current_lr:.2e}")
                    if overfitting_signal:
                        print(f"   âš ï¸  è¿‡æ‹Ÿåˆä¿¡å·æ£€æµ‹!")
                    
                    # ğŸƒ è¯æ–¹4: æ—©åœé˜ˆå€¼æ£€æŸ¥
                    if perplexity > config['early_stop_ppl']:
                        print(f"ğŸƒ [è¯æ–¹4] å›°æƒ‘åº¦ {perplexity:.2f} > {config['early_stop_ppl']}, è§¦å‘æ—©åœ")
                        cleanup()
                        return
                    
                    # ğŸƒ è¯æ–¹5: è§‚å¯Ÿè®­ç»ƒæ›²çº¿ï¼ŒåŠ¨æ€è°ƒæ•´LRå»ºè®®
                    if len(training_losses) >= 3:
                        recent_train_trend = training_losses[-1] - training_losses[-3]
                        if abs(recent_train_trend) < 0.001:
                            print(f"ğŸƒ [è¯æ–¹5] è®­ç»ƒæŸå¤±åœæ»ï¼Œå»ºè®®å‡é«˜å­¦ä¹ ç‡")
                        elif recent_train_trend > 0.01:
                            print(f"ğŸƒ [è¯æ–¹5] è®­ç»ƒæŸå¤±éœ‡è¡ï¼Œå»ºè®®é™ä½å­¦ä¹ ç‡")
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        patience_counter = 0
                        torch.save({
                            'model_state_dict': model.module.state_dict(),
                            'optimizer_state_dict': optimizer.state_dict(),
                            'scheduler_state_dict': scheduler.state_dict(),
                            'global_step': global_step,
                            'val_loss': val_loss,
                            'perplexity': perplexity,
                            'config': config,
                            'training_history': {
                                'training_losses': training_losses,
                                'validation_losses': validation_losses
                            }
                        }, f"{config['save_dir']}/best_model_anti_overfitting.pt")
                        print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {val_loss:.4f}, å›°æƒ‘åº¦: {perplexity:.2f})")
                    else:
                        patience_counter += 1
                        print(f"â° æ—©åœè®¡æ•°å™¨: {patience_counter}/{config['early_stop_patience']}")
                        
                        # ğŸƒ è¯æ–¹6: æ­£åˆ™åŠ å‘³æ£€æŸ¥ - éªŒè¯PPL<15ä¸”ä»åœ¨å¤è¯»
                        if perplexity < 15:
                            print(f"ğŸƒ [è¯æ–¹6] éªŒè¯PPL {perplexity:.2f} < 15 ä½†æŸå¤±æœªé™ï¼Œå¯èƒ½å‡ºç°å¤è¯»")
                            print(f"   å»ºè®®: å·²åº”ç”¨ attn_dropout={config['attention_dropout']} + weight_decay={config['weight_decay']}")
                        
                        # ä¸¥æ ¼çš„æ—©åœç­–ç•¥
                        if patience_counter >= config['early_stop_patience']:
                            print(f"ğŸ›‘ æ—©åœè§¦å‘: éªŒè¯æŸå¤±è¿ç»­{config['early_stop_patience']}æ¬¡æœªæ”¹å–„")
                            print(f"ğŸ¯ æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
                            cleanup()
                            return
                    
                    model.train()
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆä¸»è¿›ç¨‹ï¼‰
                if rank == 0 and global_step % config['save_steps'] == 0:
                    torch.save({
                        'model_state_dict': model.module.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'epoch': epoch,
                        'global_step': global_step,
                        'config': config
                    }, f"{config['save_dir']}/checkpoint_step_{global_step}.pt")
                    print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: step_{global_step}")
        
        # è®¡ç®—epochç»Ÿè®¡
        epoch_loss = epoch_loss / len(train_loader) * config['gradient_accum_steps']
        epoch_time = time.time() - start_time
        
        if rank == 0:
            print(f"ğŸ¯ Epoch {epoch+1}/{config['max_epochs']} å®Œæˆ")
            print(f"   å¹³å‡è®­ç»ƒæŸå¤±: {epoch_loss:.4f}")
            print(f"   è€—æ—¶: {epoch_time:.1f}ç§’")
            print(f"   å½“å‰å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.2e}")
            
            # å¦‚æœè¿™æ˜¯ç¬¬ä¸€ä¸ªepochå¹¶ä¸”å·²ç»å¾ˆå¥½äº†ï¼Œå»ºè®®æå‰åœæ­¢
            if epoch == 0 and len(validation_losses) > 0 and validation_losses[-1] < 2.0:
                print(f"ğŸ¯ æ³¨æ„: ç¬¬ä¸€ä¸ªepochéªŒè¯æŸå¤±å·²ç»å¾ˆä½ ({validation_losses[-1]:.4f})")
                print(f"   å»ºè®®è€ƒè™‘æ›´æ—©åœæ­¢ä»¥é˜²è¿‡æ‹Ÿåˆ")
    
    # æœ€ç»ˆç»Ÿè®¡
    if rank == 0:
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ¯ æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        print(f"ğŸ“Š è®­ç»ƒæ€»æ­¥æ•°: {global_step}")
        if len(validation_losses) >= 2:
            final_trend = validation_losses[-1] - validation_losses[0]
            print(f"ğŸ“ˆ éªŒè¯æŸå¤±å˜åŒ–: {final_trend:+.4f}")
    
    cleanup()

def main():
    """ä¸»å‡½æ•° - é˜²è¿‡æ‹Ÿåˆé…ç½®"""
    world_size = torch.cuda.device_count()
    print(f"ğŸš€ å¯åŠ¨é˜²è¿‡æ‹Ÿåˆè®­ç»ƒè„šæœ¬")
    print(f"ğŸ¯ æ£€æµ‹åˆ° {world_size} å¼ GPU")
    print(f"ğŸ“š åŸºäº300M tokensæ•°æ®é›†çš„1 epochè®­ç»ƒç­–ç•¥")
    
    # ğŸ”¥ é˜²è¿‡æ‹Ÿåˆè®­ç»ƒé…ç½® - ä¸¥æ ¼æŒ‰ç…§å»ºè®®
    config = {
        # æ•°æ®é…ç½®
        'data_path': 'wiki-full-zh/processed/train_seq256_bert_fast.pt',  # ä½¿ç”¨ä¿®å¤åçš„æ•°æ®
        
        # æ¨¡å‹é…ç½® (ä¿æŒåˆç†è§„æ¨¡)
        'd_model': 512,                     
        'num_transformer_layers': 6,        # å‡å°‘å±‚æ•°é˜²è¿‡æ‹Ÿåˆ
        'num_heads': 8,                     
        'num_rev_blocks': 3,                # å‡å°‘å¯é€†å—
        'num_rotors': 2,                    
        
        # ğŸ¯ å…³é”®é˜²è¿‡æ‹Ÿåˆè¶…å‚æ•°
        'max_epochs': 1,                    # ğŸ’¥ 1 epochå°±å¤Ÿ (300M tokensæ•°æ®)
        'learning_rate': 5e-4,              # ğŸƒ è¯æ–¹5: è½»è°ƒLR 1e-4â†’5e-4 (cosine decay)
        'weight_decay': 0.01,               # ğŸ’¥ æŒ‰å»ºè®®0.01
        'attention_dropout': 0.1,           # ğŸƒ è¯æ–¹6: æ­£åˆ™åŠ å‘³ attn_dropout=0.1
        
        # è®­ç»ƒé…ç½®
        'batch_size': 12,                   # ç¨å°çš„batch size
        'gradient_accum_steps': 2,          # ç­‰æ•ˆbatch size = 12*5*2 = 120
        'warmup_ratio': 0.05,               # çŸ­é¢„çƒ­ï¼Œå¿«é€Ÿåˆ°æœ€å¤§å­¦ä¹ ç‡
        
        # ğŸƒ è¯æ–¹4: æ—©åœé˜ˆå€¼PPL 20ï¼Œæ¯500æ­¥è¯„ä¼°
        'early_stop_patience': 3,           # éªŒè¯PPLä¸‰æ¬¡ä¸é™å³åœ
        'early_stop_ppl': 20,               # PPLé˜ˆå€¼ï¼Œè¶…è¿‡å³åœæ­¢
        'eval_steps': 500,                  # æ¯500æ­¥è¯„ä¼°
        'save_steps': 5000,                 
        
        # ä¿å­˜é…ç½®
        'save_dir': 'checkpoints_anti_overfitting'
    }
    
    # è®¡ç®—å…³é”®æŒ‡æ ‡
    effective_batch_size = config['batch_size'] * world_size * config['gradient_accum_steps']
    estimated_tokens_per_epoch = 1192999 * 256  # ä¿®å¤åçš„è®­ç»ƒé›†å¤§å° * åºåˆ—é•¿åº¦
    
    print(f"\nğŸ¯ é˜²è¿‡æ‹Ÿåˆè®­ç»ƒé…ç½®:")
    print(f"   ğŸ“Š æ•°æ®: ~{estimated_tokens_per_epoch/1e6:.1f}M tokens")
    print(f"   ğŸ”„ max_epochs: {config['max_epochs']} (ä¸¥æ ¼é™åˆ¶!)")
    print(f"   ğŸ“ batch_size: {config['batch_size']} Ã— {world_size} GPUs Ã— {config['gradient_accum_steps']} = {effective_batch_size}")
    print(f"   ğŸ“‰ learning_rate: {config['learning_rate']}")
    print(f"   ğŸ‹ï¸  weight_decay: {config['weight_decay']}")
    print(f"   ğŸ’§ attention_dropout: {config['attention_dropout']}")
    print(f"   â° early_stop_patience: {config['early_stop_patience']}")
    print(f"   ğŸ” eval_steps: {config['eval_steps']}")
    
    print(f"\nğŸ’¡ é¢„æœŸæ•ˆæœ:")
    print(f"   - éªŒè¯å›°æƒ‘åº¦åº”ä¿æŒåœ¨åˆç†èŒƒå›´ (2-8)")
    print(f"   - æ¨¡å‹ç”Ÿæˆåº”è¯¥è¿è´¯ä¸”æœ‰æ„ä¹‰")
    print(f"   - é¿å…éªŒè¯æŸå¤±è¿‡ä½å¯¼è‡´çš„è¿‡æ‹Ÿåˆ")
    
    # å¯åŠ¨å¤šè¿›ç¨‹è®­ç»ƒ
    mp.spawn(train_worker, args=(world_size, config), nprocs=world_size, join=True)
    
    print("ğŸ‰ é˜²è¿‡æ‹Ÿåˆè®­ç»ƒå®Œæˆ!")
    print("ğŸ’¡ å»ºè®®: æ£€æŸ¥æœ€ç»ˆæ¨¡å‹çš„å›°æƒ‘åº¦å’Œç”Ÿæˆè´¨é‡")

if __name__ == "__main__":
    main() 