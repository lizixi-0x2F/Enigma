#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import torch
import warnings
import json
from pathlib import Path

# æ·»åŠ enigmaæ¨¡å—åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from accelerate import Accelerator
from transformers import (
    TrainingArguments, 
    Trainer,
    DataCollatorForSeq2Seq,
    TrainerCallback,
    PreTrainedTokenizer,
    set_seed
)
from datasets import Dataset
from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training
import logging

# å¯¼å…¥æˆ‘ä»¬çš„è‡ªå®šä¹‰æ¨¡å‹
from enigma.modeling_enigma import EnigmaConfig, EnigmaForCausalLM

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¿½ç•¥ä¸€äº›è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

# ğŸ¯ è‡ªå®šä¹‰Lossæ—©åœå›è°ƒ
class LossEarlyStoppingCallback(TrainerCallback):
    def __init__(self, loss_threshold=0.1, patience=3):
        self.loss_threshold = loss_threshold
        self.patience = patience
        self.loss_below_threshold_count = 0
        self.should_stop = False
        
    def on_step_end(self, args, state, control, logs=None, **kwargs):
        # æ¯æ­¥ç»“æŸæ—¶æ£€æŸ¥ï¼Œä»å½“å‰batchçš„lossè®¡ç®—
        if hasattr(state, 'log_history') and len(state.log_history) > 0:
            # è·å–æœ€è¿‘çš„è®­ç»ƒloss
            recent_logs = [log for log in state.log_history if 'loss' in log]
            if recent_logs:
                current_loss = recent_logs[-1]['loss']
                
                # æ¯æ­¥æ£€æŸ¥loss
                if current_loss < self.loss_threshold:
                    self.loss_below_threshold_count += 1
                    if Accelerator.is_main_process:
                        logger.info(f"ğŸ¯ æ­¥éª¤{state.global_step}: Loss={current_loss:.4f} < {self.loss_threshold} (ç¬¬{self.loss_below_threshold_count}æ¬¡)")
                    
                    if self.loss_below_threshold_count > self.patience:
                        if Accelerator.is_main_process:
                            logger.info(f"ğŸ›‘ Losså°äº{self.loss_threshold}å·²è¿ç»­{self.loss_below_threshold_count}æ¬¡ï¼Œè§¦å‘æ—©åœï¼")
                        control.should_training_stop = True
                        self.should_stop = True
                else:
                    # å¦‚æœLossåˆä¸Šå‡äº†ï¼Œé‡ç½®è®¡æ•°å™¨
                    if self.loss_below_threshold_count > 0:
                        if Accelerator.is_main_process:
                            logger.info(f"âš ï¸ æ­¥éª¤{state.global_step}: Loss={current_loss:.4f}å›å‡ï¼Œé‡ç½®è®¡æ•°å™¨")
                        self.loss_below_threshold_count = 0

# ğŸ¯ è‡ªå®šä¹‰PPLæ—©åœå›è°ƒï¼ˆSFTä¸“ç”¨ï¼‰
class PPLEarlyStoppingCallback(TrainerCallback):
    def __init__(self, ppl_threshold=25.0, patience=2):
        self.ppl_threshold = ppl_threshold
        self.patience = patience
        self.ppl_below_threshold_count = 0
        self.should_stop = False
        
    def on_step_end(self, args, state, control, logs=None, **kwargs):
        # æ¯æ­¥ç»“æŸæ—¶æ£€æŸ¥PPL
        if hasattr(state, 'log_history') and len(state.log_history) > 0:
            # è·å–æœ€è¿‘çš„è®­ç»ƒloss
            recent_logs = [log for log in state.log_history if 'loss' in log]
            if recent_logs:
                current_loss = recent_logs[-1]['loss']
                current_ppl = torch.exp(torch.tensor(current_loss)).item()
                
                # æ¯æ­¥æ£€æŸ¥PPL
                if current_ppl <= self.ppl_threshold:
                    self.ppl_below_threshold_count += 1
                    if Accelerator.is_main_process:
                        logger.info(f"ğŸ¯ SFTæ­¥éª¤{state.global_step}: Loss={current_loss:.4f}, PPL={current_ppl:.2f} â‰¤ {self.ppl_threshold} (ç¬¬{self.ppl_below_threshold_count}æ¬¡)")
                    
                    if self.ppl_below_threshold_count > self.patience:
                        if Accelerator.is_main_process:
                            logger.info(f"ğŸ›‘ PPLâ‰¤{self.ppl_threshold}å·²è¿ç»­{self.ppl_below_threshold_count}æ¬¡ï¼ŒSFTå¾®è°ƒå®Œæˆï¼")
                        control.should_training_stop = True
                        self.should_stop = True
                else:
                    # å¦‚æœPPLåˆä¸Šå‡äº†ï¼Œé‡ç½®è®¡æ•°å™¨
                    if self.ppl_below_threshold_count > 0:
                        if Accelerator.is_main_process:
                            logger.info(f"âš ï¸ SFTæ­¥éª¤{state.global_step}: Loss={current_loss:.4f}, PPL={current_ppl:.2f}å›å‡ï¼Œé‡ç½®è®¡æ•°å™¨")
                        self.ppl_below_threshold_count = 0
                
                # é¢å¤–è®°å½•PPLè¿›å±•
                if state.global_step % 10 == 0 and Accelerator.is_main_process:
                    logger.info(f"ğŸ“Š SFTè¿›åº¦: Step {state.global_step}, Loss={current_loss:.4f}, PPL={current_ppl:.2f}")

# ğŸ”¥ å®Œæ•´Enigma Tokenizer
class EnigmaTokenizer(PreTrainedTokenizer):
    """åŸºäºçœŸå®æ•°æ®è®­ç»ƒçš„Enigma Tokenizer"""
    
    def __init__(self, vocab_file="enigma_tokenizer/vocab.json", **kwargs):
        # åŠ è½½è¯æ±‡è¡¨
        with open(vocab_file, 'r', encoding='utf-8') as f:
            self._vocab = json.load(f)
        
        self._id_to_token = {v: k for k, v in self._vocab.items()}
        
        # è®¾ç½®ç‰¹æ®Štoken
        super().__init__(
            unk_token="<unk>",
            bos_token="<s>",
            eos_token="</s>",
            pad_token="<pad>",
            **kwargs
        )
    
    @property
    def vocab_size(self):
        return len(self._vocab)
    
    def _tokenize(self, text):
        """å­—ç¬¦çº§åˆ†è¯"""
        return list(text)
    
    def _convert_token_to_id(self, token):
        return self._vocab.get(token, self._vocab.get("<unk>", 0))
    
    def _convert_id_to_token(self, index):
        return self._id_to_token.get(index, "<unk>")
    
    def get_vocab(self):
        return self._vocab
    
    def save_vocabulary(self, save_directory, filename_prefix=None):
        """ä¿å­˜è¯æ±‡è¡¨æ–‡ä»¶"""
        if filename_prefix is None:
            filename_prefix = ""
        
        vocab_file = os.path.join(save_directory, f"{filename_prefix}vocab.json")
        
        with open(vocab_file, 'w', encoding='utf-8') as f:
            json.dump(self._vocab, f, ensure_ascii=False, indent=2)
        
        return (vocab_file,)

def main():
    # è®¾ç½®éšæœºç§å­
    set_seed(42)
    
    # åˆå§‹åŒ–Accelerator
    accelerator = Accelerator()
    
    if accelerator.is_main_process:
        logger.info(f"âš¡ Enigma LoRA-SFTå¾®è°ƒ - ä½¿ç”¨ {accelerator.num_processes} ä¸ªGPU")
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹é…ç½®
    config = EnigmaConfig.from_pretrained("./output_full_sequence/final_model")
    config.max_position_embeddings = 2048
    
    if accelerator.is_main_process:
        logger.info(f"ğŸ¤– ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½: output_full_sequence/final_model")
        logger.info(f"ğŸ“Š æ¨¡å‹é…ç½®: {config.num_transformer_layers}å±‚, {config.hidden_size}ç»´")
    
    # ğŸ”¥ ä½¿ç”¨å®Œæ•´çš„Enigma Tokenizer
    if accelerator.is_main_process:
        logger.info("ğŸ”§ åŠ è½½å®Œæ•´Enigma Tokenizer...")
    
    tokenizer = EnigmaTokenizer()
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.unk_token
    
    if accelerator.is_main_process:
        logger.info(f"âœ… TokenizeråŠ è½½å®Œæˆï¼Œè¯æ±‡é‡: {tokenizer.vocab_size:,}")
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    if accelerator.is_main_process:
        logger.info("ğŸ”„ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
    
    model = EnigmaForCausalLM.from_pretrained("./output_full_sequence/final_model")
    
    if accelerator.is_main_process:
        logger.info(f"âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å®Œæˆï¼Œå‚æ•°é‡: {model.num_parameters():,}")
    
    # ğŸ¯ é…ç½®LoRA - å¯¹æ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œè¿›è¡Œå¾®è°ƒ
    if accelerator.is_main_process:
        logger.info("ğŸ” æ£€æŸ¥æ¨¡å‹å±‚ç»“æ„...")
        for name, module in model.named_modules():
            if 'transformer_blocks' in name and any(x in name for x in ['attention', 'mlp', 'in_proj', 'out_proj']):
                logger.info(f"  {name}: {type(module).__name__}")
    
    # ğŸ¯ é…ç½®LoRA - å¯¹å…³é”®å±‚è¿›è¡Œä½ç§©å¢é‡
    # æ³¨æ„åŠ›æŠ•å½±ï¼šin_proj_weightåŒ…å«Q/K/Vï¼Œout_projæ˜¯OæŠ•å½±
    # å‰é¦ˆç½‘ç»œï¼šmlp.0æ˜¯FC1ï¼Œmlp.2æ˜¯FC2
    lora_config = LoraConfig(
        r=16,  # rank
        lora_alpha=32,  # scaling parameter
        target_modules=[
            # æ³¨æ„åŠ›æŠ•å½±å±‚ (Q/K/V/O)
            "attention.in_proj_weight",    # Q/K/VæŠ•å½± (1536, 512) = 3 * (512, 512)
            "attention.out_proj",          # OæŠ•å½± (512, 512)
            # å‰é¦ˆç½‘ç»œå±‚ (FC1/FC2)  
            "mlp.0",                       # FC1 (512 -> 2048)
            "mlp.2",                       # FC2 (2048 -> 512)
            # è¾“å‡ºå±‚ä¹Ÿä¿ç•™
            "lm_head"
        ],
        lora_dropout=0.1,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    
    if accelerator.is_main_process:
        model.print_trainable_parameters()
        logger.info("ğŸ¯ LoRAé…ç½®å®Œæˆï¼Œå¼€å§‹åŠ è½½SFTæ•°æ®...")
    
    # ğŸ“š åŠ è½½SFTæ•°æ®å¹¶å¤„ç†
    def load_sft_data(file_path):
        """åŠ è½½å¹¶å¤„ç†SFTæ•°æ®"""
        data = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line.strip())
                
                # æ„å»ºå¯¹è¯æ ¼å¼ï¼šç³»ç»Ÿæç¤ºè¯ + æŒ‡ä»¤ + å›ç­”
                system_prompt = "ä½ æ˜¯Enigma"
                instruction = item.get('instruction', '')
                input_text = item.get('input', '')
                output_text = item.get('output', '')
                
                # ç»„åˆè¾“å…¥
                if input_text:
                    full_instruction = f"{instruction}\n{input_text}"
                else:
                    full_instruction = instruction
                
                # æ„å»ºè®­ç»ƒæ–‡æœ¬æ ¼å¼
                prompt = f"<s>[INST] {system_prompt}\n\n{full_instruction} [/INST] {output_text}</s>"
                
                data.append({
                    'text': prompt,
                    'instruction': full_instruction,
                    'output': output_text
                })
        
        return data
    
    # åŠ è½½æ•°æ®
    sft_data = load_sft_data("sft_data.jsonl")
    
    if accelerator.is_main_process:
        logger.info(f"ğŸ“š SFTæ•°æ®åŠ è½½å®Œæˆ: {len(sft_data):,} æ¡å¯¹è¯")
        logger.info(f"ğŸ’¡ ç¤ºä¾‹å¯¹è¯é•¿åº¦: {len(sft_data[0]['text'])} å­—ç¬¦")
    
    # æ•°æ®é¢„å¤„ç†å‡½æ•°
    def preprocess_function(examples):
        """é¢„å¤„ç†å‡½æ•°ï¼štokenizeæ–‡æœ¬ï¼ˆå­—ç¬¦çº§ï¼‰"""
        texts = [item['text'] for item in examples]
        
        tokenized = []
        for text in texts:
            # ğŸ”¥ å­—ç¬¦çº§tokenization - ç›´æ¥ä½¿ç”¨å­—ç¬¦
            chars = list(text)[:512]  # æˆªæ–­åˆ°512å­—ç¬¦è¿›è¡ŒSFT
            token_ids = [tokenizer._convert_token_to_id(char) for char in chars]
            
            tokenized.append({
                'input_ids': token_ids,
                'labels': token_ids.copy()  # å¯¹äºå› æœè¯­è¨€æ¨¡å‹ï¼Œlabelså’Œinput_idsç›¸åŒ
            })
        
        return tokenized
    
    # è½¬æ¢ä¸ºDataset
    processed_data = preprocess_function(sft_data)
    train_dataset = Dataset.from_list(processed_data)
    
    # åˆ›å»ºæ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding=True,
        return_tensors="pt"
    )
    
    # åˆ›å»ºoutputç›®å½•
    if accelerator.is_main_process:
        os.makedirs("output_lora_sft", exist_ok=True)
    
    # ğŸ¯ SFTè®­ç»ƒå‚æ•°ï¼ˆè¾ƒå°æ‰¹æ¬¡ï¼Œé€‚åˆå¾®è°ƒï¼‰
    training_args = TrainingArguments(
        output_dir="output_lora_sft",
        per_device_train_batch_size=4,  # è¾ƒå°æ‰¹æ¬¡é€‚åˆSFT
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=8,  # æœ‰æ•ˆæ‰¹æ¬¡=4*8*5=160
        num_train_epochs=3,
        eval_strategy="no",  # ç¦ç”¨è¯„ä¼°ç­–ç•¥
        save_steps=999999,  # ç¦ç”¨ä¸­é—´checkpoint
        save_total_limit=1,
        logging_steps=10,  # æ›´é¢‘ç¹çš„æ—¥å¿—
        fp16=True,
        dataloader_num_workers=4,
        dataloader_pin_memory=True,
        load_best_model_at_end=False,
        report_to=["tensorboard"],
        warmup_steps=50,
        learning_rate=1e-4,  # SFTä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡
        weight_decay=0.01,
        lr_scheduler_type="cosine",
        save_safetensors=False,
        gradient_checkpointing=False,
        remove_unused_columns=False,
        # åˆ†å¸ƒå¼ä¼˜åŒ–
        ddp_find_unused_parameters=False,
        ddp_broadcast_buffers=False,
        ddp_bucket_cap_mb=100,
        # æ€§èƒ½ä¼˜åŒ–
        fp16_full_eval=True,
        max_steps=1000,  # SFTä¸éœ€è¦å¤ªå¤šæ­¥éª¤
    )
    
    # è¶…é«˜æ€§èƒ½SFT Trainerç±»
    class UltraFastSFTTrainer(Trainer):
        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
            labels = inputs.get("labels")
            
            # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå‘é‡åŒ–Enigmaå¤„ç†ï¼ˆåŒé¢„è®­ç»ƒï¼‰
            input_ids = inputs.get("input_ids")
            batch_size, seq_length = input_ids.shape
            
            # å¤„ç†DistributedDataParallelåŒ…è£…çš„æ¨¡å‹
            actual_model = model.module if hasattr(model, 'module') else model
            
            # LoRAæ¨¡å‹éœ€è¦é€šè¿‡base_modelè®¿é—®
            if hasattr(actual_model, 'base_model'):
                base_model = actual_model.base_model.model
            else:
                base_model = actual_model
            
            # 1. TokenåµŒå…¥
            hidden_states = base_model.token_embedding(input_ids)  # [B, L, d]
            
            # 2. å‘é‡åŒ–Enigmaå¤„ç†
            flat_hidden = hidden_states.view(-1, hidden_states.size(-1))
            flat_enigma_out = base_model.enigma(flat_hidden)
            hidden_states = flat_enigma_out.view(batch_size, seq_length, -1)
            
            # 3. é€šè¿‡Transformerå±‚
            for transformer_block in base_model.transformer_blocks:
                hidden_states = transformer_block(hidden_states)
            
            # 4. è¾“å‡ºå±‚
            hidden_states = base_model.output_norm(hidden_states)
            logits = base_model.lm_head(hidden_states)
            
            # 5. è®¡ç®—æŸå¤±
            if labels is not None:
                loss_fct = torch.nn.CrossEntropyLoss()
                shift_logits = logits[..., :-1, :].contiguous()
                shift_labels = labels[..., 1:].contiguous()
                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
            else:
                loss = None
            
            return (loss, logits) if return_outputs else loss
        
        def log(self, logs, start_time=None):
            if accelerator.is_main_process and "train_loss" in logs:
                step = self.state.global_step
                logger.info(f"âš¡ SFTæ­¥éª¤ {step}: loss={logs['train_loss']:.4f}")
            
            super().log(logs)
    
    # åˆ›å»ºLossæ—©åœå›è°ƒï¼ˆSFTé€šå¸¸lossä¼šé™å¾—å¾ˆä½ï¼‰
    loss_callback = LossEarlyStoppingCallback(loss_threshold=0.1, patience=3)
    
    # åˆ›å»ºPPLæ—©åœå›è°ƒï¼ˆSFTä¸“ç”¨ï¼‰
    ppl_callback = PPLEarlyStoppingCallback(ppl_threshold=25.0, patience=2)
    
    # åˆ›å»ºTrainer
    trainer = UltraFastSFTTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
        callbacks=[loss_callback, ppl_callback],
    )
    
    # å¼€å§‹SFTè®­ç»ƒ
    if accelerator.is_main_process:
        logger.info("âš¡âš¡âš¡ å¼€å§‹Enigma LoRA-SFTå¾®è°ƒ...")
        logger.info(f"ğŸ“Š è®­ç»ƒæ•°æ®: {len(sft_data):,} æ¡")
        logger.info(f"ğŸ“Š æœ‰æ•ˆæ‰¹æ¬¡: {4 * 8 * accelerator.num_processes}")
        logger.info(f"ğŸ“Š åºåˆ—é•¿åº¦: 512 (SFTä¼˜åŒ–)")
        logger.info(f"ğŸ¯ ç›®æ ‡: Loss < 0.1, PPL â‰¤ 25")
    
    try:
        import time
        start_time = time.time()
        
        train_result = trainer.train()
        
        end_time = time.time()
        total_time = end_time - start_time
        
        if accelerator.is_main_process:
            steps_per_second = trainer.state.global_step / total_time
            logger.info(f"ğŸ‰ SFTå¾®è°ƒå®Œæˆï¼")
            logger.info(f"âš¡ æ€»ç”¨æ—¶: {total_time:.2f}ç§’")
            logger.info(f"âš¡ è®­ç»ƒé€Ÿåº¦: {steps_per_second:.3f} steps/ç§’")
            logger.info(f"âš¡ å¹³å‡æ¯æ­¥: {total_time/trainer.state.global_step:.1f}ç§’")
            
            # ä¿å­˜LoRAé€‚é…å™¨
            model.save_pretrained("output_lora_sft/lora_adapter")
            tokenizer.save_pretrained("output_lora_sft/lora_adapter")
            
            logger.info("ğŸ’¾ LoRAé€‚é…å™¨å·²ä¿å­˜åˆ° output_lora_sft/lora_adapter")
        
    except KeyboardInterrupt:
        if accelerator.is_main_process:
            logger.info("â¹ï¸ SFTè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            model.save_pretrained("output_lora_sft/interrupted_adapter")
    
    except Exception as e:
        if accelerator.is_main_process:
            logger.error(f"âŒ SFTè®­ç»ƒå‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        raise

if __name__ == "__main__":
    main() 