#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import torch
import warnings
from pathlib import Path
import json

# æ·»åŠ enigmaæ¨¡å—åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from accelerate import Accelerator
from transformers import (
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling,
    EarlyStoppingCallback,
    TrainerCallback,
    PreTrainedTokenizer,
    set_seed
)
from datasets import load_from_disk
import logging

# å¯¼å…¥æˆ‘ä»¬çš„è‡ªå®šä¹‰æ¨¡å‹
from enigma.modeling_enigma import EnigmaConfig, EnigmaForCausalLM

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¿½ç•¥ä¸€äº›è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

# ğŸ¯ è‡ªå®šä¹‰PPLæ—©åœå›è°ƒ
class PPLEarlyStoppingCallback(TrainerCallback):
    def __init__(self, ppl_threshold=25, patience=2):
        self.ppl_threshold = ppl_threshold
        self.patience = patience
        self.ppl_below_threshold_count = 0
        self.should_stop = False
        
    def on_step_end(self, args, state, control, logs=None, **kwargs):
        # æ¯æ­¥ç»“æŸæ—¶æ£€æŸ¥ï¼Œä»å½“å‰batchçš„lossè®¡ç®—PPL
        if hasattr(state, 'log_history') and len(state.log_history) > 0:
            # è·å–æœ€è¿‘çš„è®­ç»ƒloss
            recent_logs = [log for log in state.log_history if 'loss' in log]
            if recent_logs:
                current_loss = recent_logs[-1]['loss']
                current_ppl = torch.exp(torch.tensor(current_loss))
                
                # æ¯æ­¥æ£€æŸ¥PPL
                if current_ppl < self.ppl_threshold:
                    self.ppl_below_threshold_count += 1
                    if Accelerator.is_main_process:
                        logger.info(f"ğŸ¯ æ­¥éª¤{state.global_step}: PPL={current_ppl:.2f} < {self.ppl_threshold} (ç¬¬{self.ppl_below_threshold_count}æ¬¡)")
                    
                    if self.ppl_below_threshold_count > self.patience:
                        if Accelerator.is_main_process:
                            logger.info(f"ğŸ›‘ PPLå°äº{self.ppl_threshold}å·²è¿ç»­{self.ppl_below_threshold_count}æ¬¡ï¼Œè§¦å‘æ—©åœï¼")
                        control.should_training_stop = True
                        self.should_stop = True
                else:
                    # å¦‚æœPPLåˆä¸Šå‡äº†ï¼Œé‡ç½®è®¡æ•°å™¨
                    if self.ppl_below_threshold_count > 0:
                        if Accelerator.is_main_process:
                            logger.info(f"âš ï¸ æ­¥éª¤{state.global_step}: PPL={current_ppl:.2f}å›å‡ï¼Œé‡ç½®è®¡æ•°å™¨")
                        self.ppl_below_threshold_count = 0

# ğŸ”¥ å®Œæ•´Enigma Tokenizer
class EnigmaTokenizer(PreTrainedTokenizer):
    """åŸºäºçœŸå®æ•°æ®è®­ç»ƒçš„Enigma Tokenizer"""
    
    def __init__(self, vocab_file="enigma_tokenizer/vocab.json", **kwargs):
        # åŠ è½½è¯æ±‡è¡¨
        with open(vocab_file, 'r', encoding='utf-8') as f:
            self._vocab = json.load(f)
        
        self._id_to_token = {v: k for k, v in self._vocab.items()}
        
        # è®¾ç½®ç‰¹æ®Štoken
        super().__init__(
            unk_token="<unk>",
            bos_token="<s>",
            eos_token="</s>",
            pad_token="<pad>",
            **kwargs
        )
    
    @property
    def vocab_size(self):
        return len(self._vocab)
    
    def _tokenize(self, text):
        """å­—ç¬¦çº§åˆ†è¯"""
        return list(text)
    
    def _convert_token_to_id(self, token):
        return self._vocab.get(token, self._vocab.get("<unk>", 0))
    
    def _convert_id_to_token(self, index):
        return self._id_to_token.get(index, "<unk>")
    
    def get_vocab(self):
        return self._vocab
    
    def save_vocabulary(self, save_directory, filename_prefix=None):
        """ä¿å­˜è¯æ±‡è¡¨æ–‡ä»¶"""
        if filename_prefix is None:
            filename_prefix = ""
        
        vocab_file = os.path.join(save_directory, f"{filename_prefix}vocab.json")
        
        with open(vocab_file, 'w', encoding='utf-8') as f:
            json.dump(self._vocab, f, ensure_ascii=False, indent=2)
        
        return (vocab_file,)

def main():
    # è®¾ç½®éšæœºç§å­
    set_seed(42)
    
    # åˆå§‹åŒ–Accelerator
    accelerator = Accelerator()
    
    if accelerator.is_main_process:
        logger.info(f"âš¡ è¶…çº§ä¼˜åŒ–è®­ç»ƒ - ä½¿ç”¨ {accelerator.num_processes} ä¸ªGPU")
    
    # åŠ è½½é…ç½®å¹¶ä¿æŒåŸå§‹åºåˆ—é•¿åº¦
    config = EnigmaConfig.from_pretrained("./config.json")
    # ğŸ”¥ ä½¿ç”¨å®Œæ•´åºåˆ—é•¿åº¦2048ï¼Œä¸æˆªæ–­
    config.max_position_embeddings = 2048
    
    if accelerator.is_main_process:
        logger.info(f"ğŸ”¥ å®Œæ•´åºåˆ—è®­ç»ƒ: 2048 (æ— æˆªæ–­)")
        logger.info(f"æ¨¡å‹é…ç½®: {config.num_transformer_layers}å±‚, {config.hidden_size}ç»´")
    
    # ğŸ”¥ ä½¿ç”¨å®Œæ•´çš„Enigma Tokenizer
    if accelerator.is_main_process:
        logger.info("ğŸ”§ åŠ è½½å®Œæ•´Enigma Tokenizer...")
    
    tokenizer = EnigmaTokenizer()
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.unk_token
    
    if accelerator.is_main_process:
        logger.info(f"âœ… TokenizeråŠ è½½å®Œæˆï¼Œè¯æ±‡é‡: {tokenizer.vocab_size:,}")
    
    # åˆ›å»ºæ¨¡å‹
    if accelerator.is_main_process:
        logger.info("åˆ›å»ºæ¨¡å‹ä¸­...")
    
    model = EnigmaForCausalLM(config)
    
    if accelerator.is_main_process:
        logger.info(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆï¼Œå‚æ•°é‡: {model.num_parameters():,}")
    
    # åŠ è½½å®Œæ•´æ•°æ®é›†ï¼Œä¸æˆªæ–­
    if accelerator.is_main_process:
        logger.info("åŠ è½½å®Œæ•´æ•°æ®é›†ï¼ˆä¸æˆªæ–­åºåˆ—ï¼‰...")
    
    dataset = load_from_disk("am_0.9M_processed_hf")
    
    # ç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼Œä¸æˆªæ–­
    train_ds = dataset["train"]
    eval_ds = dataset["validation"]
    
    if accelerator.is_main_process:
        logger.info(f"âœ… è®­ç»ƒé›†: {len(train_ds):,} æ ·æœ¬ (åºåˆ—é•¿åº¦: 2048)")
        logger.info(f"âœ… éªŒè¯é›†: {len(eval_ds):,} æ ·æœ¬ (åºåˆ—é•¿åº¦: 2048)")
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, 
        mlm=False,
        return_tensors="pt"
    )
    
    # åˆ›å»ºoutputç›®å½•
    if accelerator.is_main_process:
        os.makedirs("output_full_sequence", exist_ok=True)
    
    # ğŸ”¥ è°ƒæ•´æ‰¹æ¬¡å¤§å°ä»¥é€‚åº”2048åºåˆ—é•¿åº¦
    # åºåˆ—é•¿åº¦4å€å¢åŠ ï¼Œéœ€è¦å‡å°æ‰¹æ¬¡ä»¥é¿å…OOM
    per_device_batch_size = 8   # ä»32å‡å°‘åˆ°8
    gradient_accumulation = 4   # ä»2å¢åŠ åˆ°4ï¼Œä¿æŒæœ‰æ•ˆæ‰¹æ¬¡ä¸å˜
    
    if accelerator.is_main_process:
        effective_batch = per_device_batch_size * gradient_accumulation * accelerator.num_processes
        logger.info(f"âš¡ å®Œæ•´åºåˆ—æ‰¹æ¬¡é…ç½®:")
        logger.info(f"  - æ¯è®¾å¤‡æ‰¹æ¬¡: {per_device_batch_size}")
        logger.info(f"  - æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation}")
        logger.info(f"  - GPUæ•°é‡: {accelerator.num_processes}")
        logger.info(f"  - æœ‰æ•ˆæ‰¹æ¬¡: {effective_batch}")
        logger.info(f"  - åºåˆ—é•¿åº¦: 2048 (å®Œæ•´)")
    
    # âš¡ é«˜æ€§èƒ½è®­ç»ƒå‚æ•°ï¼ˆé€‚é…å®Œæ•´åºåˆ—ï¼‰
    training_args = TrainingArguments(
        output_dir="output_full_sequence",
        per_device_train_batch_size=per_device_batch_size,    # 8ï¼Œé€‚åº”2048åºåˆ—
        per_device_eval_batch_size=per_device_batch_size,
        gradient_accumulation_steps=gradient_accumulation,
        num_train_epochs=3,
        eval_strategy="steps",
        eval_steps=500,
        save_steps=999999,  # ğŸ—‘ï¸ ç¦ç”¨ä¸­é—´checkpointï¼Œè®¾ç½®è¶…å¤§å€¼
        save_total_limit=1,  # åªä¿ç•™1ä¸ªcheckpoint
        logging_steps=25,
        fp16=True,
        dataloader_num_workers=6,
        dataloader_pin_memory=True,
        dataloader_prefetch_factor=3,
        load_best_model_at_end=False,
        report_to=["tensorboard"],
        warmup_steps=200,
        learning_rate=3e-4,
        weight_decay=0.01,
        lr_scheduler_type="cosine",
        save_safetensors=False,              # ğŸ”¥ ä¿®å¤æƒé‡å…±äº«ä¿å­˜é—®é¢˜
        gradient_checkpointing=False,    # ä¿æŒå…³é—­è·å¾—æœ€å¤§é€Ÿåº¦
        remove_unused_columns=False,
        # ğŸ”¥ åˆ†å¸ƒå¼ä¼˜åŒ–
        ddp_find_unused_parameters=False,
        ddp_broadcast_buffers=False,
        ddp_bucket_cap_mb=100,
        # ğŸ”¥ æ€§èƒ½ä¼˜åŒ–
        fp16_full_eval=True,
        max_steps=5000,
    )
    
    # è¶…é«˜æ€§èƒ½Trainerç±»
    class UltraFastEnigmaTrainer(Trainer):
        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
            labels = inputs.get("labels")
            
            # ğŸ”¥ğŸ”¥ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå‘é‡åŒ–Enigmaå¤„ç†
            input_ids = inputs.get("input_ids")
            batch_size, seq_length = input_ids.shape
            
            # å¤„ç†DistributedDataParallelåŒ…è£…çš„æ¨¡å‹
            actual_model = model.module if hasattr(model, 'module') else model
            
            # 1. TokenåµŒå…¥
            hidden_states = actual_model.token_embedding(input_ids)  # [B, L, d]
            
            # 2. å‘é‡åŒ–Enigmaå¤„ç† - å°†æ‰€æœ‰ä½ç½®å±•å¹³ä¸€æ¬¡æ€§å¤„ç†
            # åŸæ¥ï¼šfor i in range(seq_length): enigma(x[:, i])
            # ä¼˜åŒ–ï¼šenigma(x.view(-1, d)).view(B, L, d)
            flat_hidden = hidden_states.view(-1, hidden_states.size(-1))  # [B*L, d]
            flat_enigma_out = actual_model.enigma(flat_hidden)  # [B*L, d] - ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ä½ç½®ï¼
            hidden_states = flat_enigma_out.view(batch_size, seq_length, -1)  # [B, L, d]
            
            # 3. é€šè¿‡Transformerå±‚
            for transformer_block in actual_model.transformer_blocks:
                hidden_states = transformer_block(hidden_states)
            
            # 4. è¾“å‡ºå±‚
            hidden_states = actual_model.output_norm(hidden_states)
            logits = actual_model.lm_head(hidden_states)  # [B, L, vocab_size]
            
            # 5. è®¡ç®—æŸå¤±
            if labels is not None:
                loss_fct = torch.nn.CrossEntropyLoss()
                shift_logits = logits[..., :-1, :].contiguous()
                shift_labels = labels[..., 1:].contiguous()
                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
            else:
                loss = None
            
            return (loss, logits) if return_outputs else loss
        
        def log(self, logs, start_time=None):
            if accelerator.is_main_process and "train_loss" in logs:
                step = self.state.global_step
                
                # è¯¦ç»†æ€§èƒ½ç›‘æ§
                if hasattr(self, '_step_times'):
                    import time
                    current_time = time.time()
                    if len(self._step_times) > 0:
                        avg_step_time = (current_time - self._step_times[0]) / len(self._step_times)
                        logger.info(f"âš¡ æ­¥éª¤ {step}: loss={logs['train_loss']:.4f}, å¹³å‡æ­¥æ—¶={avg_step_time:.1f}s")
                    self._step_times.append(current_time)
                    if len(self._step_times) > 10:  # åªä¿ç•™æœ€è¿‘10æ­¥
                        self._step_times.pop(0)
                else:
                    self._step_times = []
            
            super().log(logs)
    
    # åˆ›å»ºPPLæ—©åœå›è°ƒ
    ppl_callback = PPLEarlyStoppingCallback(ppl_threshold=25, patience=2)
    
    # åˆ›å»ºTrainer
    trainer = UltraFastEnigmaTrainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=eval_ds,
        data_collator=data_collator,
        tokenizer=tokenizer,
        callbacks=[ppl_callback],  # ğŸ¯ æ·»åŠ PPLæ—©åœå›è°ƒ
    )
    
    # å¼€å§‹è®­ç»ƒ
    if accelerator.is_main_process:
        logger.info("âš¡âš¡âš¡ å¼€å§‹è¶…çº§ä¼˜åŒ–è®­ç»ƒ...")
        logger.info(f"ğŸ“Š åºåˆ—é•¿åº¦: 2048 (å®Œæ•´)")
        logger.info(f"ğŸ“Š æœ‰æ•ˆæ‰¹æ¬¡: {per_device_batch_size * gradient_accumulation * accelerator.num_processes}")
        logger.info(f"ğŸ“Š é¢„æœŸæ¯æ­¥æ—¶é—´: ~15-30ç§’")
        logger.info(f"ğŸ¯ ç›®æ ‡: PPL â‰¤ 25")
    
    try:
        import time
        start_time = time.time()
        
        train_result = trainer.train()
        
        end_time = time.time()
        total_time = end_time - start_time
        
        if accelerator.is_main_process:
            steps_per_second = trainer.state.global_step / total_time
            logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼")
            logger.info(f"âš¡ æ€»ç”¨æ—¶: {total_time:.2f}ç§’")
            logger.info(f"âš¡ è®­ç»ƒé€Ÿåº¦: {steps_per_second:.3f} steps/ç§’")
            logger.info(f"âš¡ å¹³å‡æ¯æ­¥: {total_time/trainer.state.global_step:.1f}ç§’")
            
            # ä¿å­˜æ¨¡å‹
            trainer.save_model("output_full_sequence/final_model")
            tokenizer.save_pretrained("output_full_sequence/final_model")
        
    except KeyboardInterrupt:
        if accelerator.is_main_process:
            logger.info("â¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            trainer.save_model("output_full_sequence/interrupted_model")
    
    except Exception as e:
        if accelerator.is_main_process:
            logger.error(f"âŒ è®­ç»ƒå‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        raise

if __name__ == "__main__":
    main() 